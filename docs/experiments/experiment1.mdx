---

id: experiment1
title: Standard Search vs. Stemming
sidebar_label: Stemming

---
import useBaseUrl from '@docusaurus/useBaseUrl'

For our first experiment we connected a Google Colab Notebook to our Elasticsearch instance and compared the standard Elasticsearch operator with two build-in search metrics.

Therefore, we parsed [these 8 free available corpora](http://ir.dcs.gla.ac.uk/resources/test_collections/) hosted by the University of Glasgow and feed them to the Elasticsearch [Ranking API](https://www.elastic.co/guide/en/elasticsearch/reference/current/search-rank-eval.html). With [our parsing guide](https://pragmalingu.de/docs/guides/how-to-parse) you can try this yourself or you can look at our [Data Set Notebooks.]()

These are the two build-in methods we compared to the standard Elasticsearch operator:

* [Standard English Stemmer](https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-stemmer-tokenfilter.html)
* [Hunspell Token Filter](https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-hunspell-tokenfilter.html)

## Standard Elasticsearch

At first we implemented a standard Elasticsearch using [multi match query](https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-simple-query-string-query.html) which searches through the field 'text' and 'title'. We only looked at the first 20 retrieved documents when messuring precision and recall. 

Here you can see the metrics we choose and an example request of the medline corpus:

```python
{
    "metric": {
        "recall": {
            "k": 20,
            "relevant_rating_threshold": 1
        }
    },
    "request": {
        "query": {
            "multi_match": {
                "fields": [
                    "title",
                    "text"
                ],
                "query": "palliation (temporary improvement) of cancer patients by using drugs, x-ray, surgery."
            }
        }
    }
}
```

These are results without any further processing of the data other than the standards of Elasticsearch:

|  standard search |   cranfield |   CISI |   ADI |   medline |   CACM |   LISA |   Time |   NPL |
|------------------|-------------|--------|-------|-----------|--------|--------|--------|-------|
| recall           |        0.15 |  0.103 | 0.328 |     0.032 |  0.034 |  0.337 |  0.772 | 0.233 |
| precision        |       0.056 |  0.154 | 0.074 |     0.031 |  0.022 |  0.157 |  0.15  | 0.217 |
| f1-score         |       0.081 |  0.123 | 0.12  |     0.032 |  0.027 |  0.214 |  0.251 | 0.225 |

## Stemming

Elasticsearch provides two build-in stemming methods. 
First of, is the algorithmic stemming. It applies a series of rules to each word to reduce them, called [stemmer token filter](https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-stemmer-tokenfilter.html) in Elasticsearch. 

Second of, the dictionary stemming replaces unstemmed words with stemmed variants from a provided dictionary. The Elasticsearch build-in conterpart is called [hunspell token filter](https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-hunspell-tokenfilter.html). 

### Stemmer Token Filter

The *stemmer token filter* supports different languages and since our corpora where all in English we used the English stemmer for our experiment.
Since it needs less memory and is way faster than a dictionary approach, in most cases it's the better way to go with this one. The implementation in Elasticsearch is also not very complicated and can be done very quickly.
Irregular words or names could cause strange forms that doesn't give any helpful information since it is only rule based.

Yet we expected an improvement to the standard Elasticsearch operator.

This is how we implemented the stemming analyzer:

```python
stemming_analyser = {
    "filter": {
        "eng_stemmer": {
            "type": "stemmer",
            "name": "english"
        }
    },
    "analyzer": {
        "default": {
            "tokenizer": "standard",
            "filter": ["lowercase", "eng_stemmer"]
        }
    }
}
```

Afterwards we incorporated it into "settings" to index the documents of each corpus with it:

```python
settings = {
    "settings": {
        "number_of_shards": 1,
        "number_of_replicas": 0,
        "analysis": stemming_analyser
    }
}

stemming_prefix = "stemming-"
```

| stemming search |   cranfield |   CISI |   ADI |   medline |   CACM |   LISA |   Time |   NPL |
|-----------------|-------------|--------|-------|-----------|--------|--------|--------|-------|
| recall          |        0.15 |  0.121 | 0.254 |     0.032 |  0.044 |  0.393 |  0.78  | 0.285 |
| precision       |       0.057 |  0.169 | 0.063 |     0.031 |  0.028 |  0.191 |  0.152 | 0.258 |
| f1-score        |       0.082 |  0.141 | 0.101 |     0.032 |  0.034 |  0.257 |  0.255 | 0.271 |

Surprisingly, the results aren't as different as expected with using the *stemmer token filter*.

### Hunspell Token Filter

Although the * hunspell token filter * from Elasticsearch recognizes irregular words and should therefore preprocess them better, we must keep in mind that words that do not appear in the dictionary are not processed. But in addition, a large dictionary naturally also takes up significantly more time and memory.
Since dictionaries can never be fully complete, we didn't expect a big change int the results comparing to the * stemmer token filter *. 

The procedure of the * hunspell token filter * is similar to a lemmatization, but the words are not always reduced to the lemma with hunspell. For example, with the word "lovingly" the stemmer returns "loving" where the lemma would be "love". So the meaning of the words is not sufficiently addressed.

First of, we implemented the hunspell stemming analyzer:



Second of, we had to implement the dictionary in our node system:




| hunspell        |   cranfield |   CISI |   ADI |   medline |   CACM |   LISA |   Time |   NPL |
|-----------------|-------------|--------|-------|-----------|--------|--------|--------|-------|
| recall          |             |        |       |           |        |        |        |       |
| precision       |             |        |       |           |        |        |        |       |
| f1-score        |             |        |       |           |        |        |        |       |

## Results

For our result we messured Recall, Precision and F1-Score for every method.
([Read more about Recall, Precision and F-Score](https://pragmalingu.de/docs/guides/basic-definitions))

**Recall**
<img alt="Recall" src={useBaseUrl('img/Recall.png')} />


**Precision**
<img alt="Precision" src={useBaseUrl('img/Precision.png')} />


**F1-Score**
<img alt="F1-Score" src={useBaseUrl('img/F1-Score.png')} />

