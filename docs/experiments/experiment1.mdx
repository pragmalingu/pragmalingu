---

id: experiment1
title: Standard Search vs. Stemming
sidebar_label: Stemming

---
import useBaseUrl from '@docusaurus/useBaseUrl'

For our first experiment we connected a Google CoLab Notebook to our Elasticsearch instance and compared the standard Elasticsearch operator with two build-in search metrics.

Therefore, we parsed [these 8 free available corpora](http://ir.dcs.gla.ac.uk/resources/test_collections/) hosted by the University of Glasgow and feed them to the Elasticsearch [Ranking API](https://www.elastic.co/guide/en/elasticsearch/reference/current/search-rank-eval.html). With [our parsing guide](https://pragmalingu.de/docs/guides/how-to-parse) you can try this yourself or you can look at our [Data Set Notebooks.]()

These are the two build-in methods we compared to the standard Elasticsearch operator:

* [Standard English Stemmer](https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-stemmer-tokenfilter.html)
* [Hunspell Token Filter](https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-hunspell-tokenfilter.html)

## Standard Elasticsearch

At first we implemented a standard Elasticsearch using [multi match query](https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-simple-query-string-query.html) which searches through the field 'text' and 'title'. We only looked at the first 20 retrieved documents when messuring precision and recall. 

Here you can see the metrics we choose and an example request of the medline corpus:

```Json
{   "metric": {
        "recall": {
        "k": 20,
        "relevant_rating_threshold": 1
        }
      },
    "request": {
                "query": {
                    "multi_match": {
                        "fields": [
                            "title",
                            "text"
                        ],
                        "query": "palliation (temporary improvement) of cancer patients by using drugs, x-ray, surgery."
                    }
                }
            }
        },
}
```
|  standard search |   cranfield |   CISI |   ADI |   medline |   CACM |   LISA |   Time |   NPL |
|------------------|-------------|--------|-------|-----------|--------|--------|--------|-------|
| recall           |        0.15 |  0.103 | 0.328 |     0.032 |  0.034 |  0.337 |  0.772 | 0.233 |
| precision        |       0.056 |  0.154 | 0.074 |     0.031 |  0.022 |  0.157 |  0.15  | 0.217 |
| f1-score         |       0.081 |  0.123 | 0.12  |     0.032 |  0.027 |  0.214 |  0.251 | 0.225 |


## Stemming

Elasticsearch provides two build-in stemming methods. 
([Click here to read more about stemming in general](https://pragmalingu.de/docs/guides/basic-definitions#stemming)) 

First of the [stemmer token filter](https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-stemmer-tokenfilter.html) which is  a algorithmic stemming method. It applies a series of rules to each word to stem them. 

The second stemming method is the [hunspell token filter](https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-hunspell-tokenfilter.html) which uses a dictionary to stem the words. 


The *stemmer token filter* provides different languages and since our corpora where all in english we used the english stemmer for our experiment.

We implemented the stemming analyzer:

```Json
stemming_analyser = {
    "filter" : {
        "eng_stemmer" : {
        "type" : "stemmer",
        "name" : "english"
        }
    },
    "analyzer" : {
        "default" : {
            "tokenizer" : "standard",
            "filter" : ["lowercase", "eng_stemmer"]
        }
    }
}
```

And incorporated it into the settings for indexing the documents of each corpus:

```Json
settings = {
    "settings": {
        "number_of_shards": 1,
        "number_of_replicas": 0,
        "analysis": stemming_analyser
    }
}

stemming_prefix = "stemming-"
```

| stemming search |   cranfield |   CISI |   ADI |   medline |   CACM |   LISA |   Time |   NPL |
|-----------------|-------------|--------|-------|-----------|--------|--------|--------|-------|
| recall          |        0.15 |  0.121 | 0.254 |     0.032 |  0.044 |  0.393 |  0.78  | 0.285 |
| precision       |       0.057 |  0.169 | 0.063 |     0.031 |  0.028 |  0.191 |  0.152 | 0.258 |
| f1-score        |       0.082 |  0.141 | 0.101 |     0.032 |  0.034 |  0.257 |  0.255 | 0.271 |

## Results

For our result we run those three methods on our corpora. We messured Recall, Precision and F1-Score.
([Read more about Recall, Precision and F-Score](https://pragmalingu.de/docs/guides/basic-definitions))

**Recall**
<img alt="Recall" src={useBaseUrl('img/Recall.png')} />



**Precision**
<img alt="Precision" src={useBaseUrl('img/Precision.png')} />



**F1-Score**
<img alt="F1-Score" src={useBaseUrl('img/F1-Score.png')} />



As you can see, the stemmer doesn't have a significant impact on all the results. The next step would be to try other operators as tokenization and lemmatization.