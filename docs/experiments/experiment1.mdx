---
id: experiment1
title: Stemming Experiment
sidebar_label: Stemming
custom_edit_url: null
---

import useBaseUrl from '@docusaurus/useBaseUrl'
import styles from '../doc.css';

For our first experiment we created a Google Colab Notebook that runs an Elasticsearch instance and compared the standard Elasticsearch analyzer with two built-in stemming methods. 
To do this, we parsed and indexed [these 8 publically available datasets](http://ir.dcs.gla.ac.uk/resources/test_collections/) provided by the University of Glasgow with the different analyzers 
and evaluated them with the Elasticsearch [Ranking Evaluation API](https://www.elastic.co/guide/en/elasticsearch/reference/current/search-rank-eval.html). 
[Our Parsing Guide](https://pragmalingu.de/docs/guides/how-to-parse) shows you how these datasets are structured and how we preprocessed them for indexing.
The complete code and workflow to arrive at the results presented here is available in our [Experiment Notebook](https://colab.research.google.com/github/pragmalingu/experiments/blob/master/01_Stemming/Experiment/First_Experiment_Stemming.ipynb) where you can reproduce everything step by step.

These are the two built-in methods that we compared to the [standard Elasticsearch analyzer](https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-standard-analyser.html):

* [Standard English Stemmer](https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-stemmer-tokenfilter.html) 
* [Hunspell Token Filter](https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-hunspell-tokenfilter.html) 

### 1. Standard Elasticsearch Analyzer

We ran our searches witht the [multi-match query](https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-multi-match-query.html). 
It searches on multiple fields of the documents - in our case 'text' and 'title' and by default uses the highest matching score from one field to rank the document.

We evaluated the ranking on the first 20 highest ranked documents returned by each query.

Here you can see the metrics we chose and an example request of the ADI corpus:

```python
{
    "metric": {
        "recall": {
            "k": 20,
            "relevant_rating_threshold": 1
        }
    },
    "request": {
        "query": {
            "multi_match": {
                "fields": [
                    "title",
                    "text"
                ],
                "query": "How can actually pertinent data, as opposed to references or entire articles themselves, be retrieved automatically in response to information requests?"
            }
        }
    }
}
```

These were the results without any further processing of the data using the multi-match query of Elasticsearch:

|  Standard Search |   Cranfield |   CISI |   ADI |   Medline |   CACM |   LISA |   Time |   NPL |
|------------------|-------------|--------|-------|-----------|--------|--------|--------|-------|
| Recall           |        0.15 |  0.103 | 0.328 |     0.032 |  0.034 |  0.354 |  0.772 | 0.233 |
| Precision        |       0.056 |  0.154 | 0.074 |     0.031 |  0.022 |  0.193 |  0.15  | 0.217 |
| F1-Score         |       0.081 |  0.123 | 0.12  |     0.032 |  0.027 |  0.25  |  0.251 | 0.225 |

### 2. Stemming

To experiment with stemming Elasticsearch provides two built-in stemming methods. 
First, is the algorithmic stemmer; it applies a series of rules to each word to reduce them to their stems. In Elasticsearch this is called the [Stemmer Token Filter](https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-stemmer-tokenfilter.html).  In addition, there is the dictionary stemmer, which replaces unstemmed words with stemmed variants from a provided dictionary. The Elasticsearch built-in method is called the [Hunspell Token Filter](https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-hunspell-tokenfilter.html). 

<details>
<summary>What is "Stemming"?</summary>  

The goal of stemming is to remove all morphological features from a word and create truncated, ambiguous **Stems**. Therefore it tries to strip off suffixes at the end of a word.<br></br>
For example, `learning` changes to `learn` after stemming. <br></br>
If the stemmer cuts off too much information, the word could become too short and lose semantic meaning; this is called overstemming.
</details>  

#### 2.1. Stemmer Token Filter

The *Stemmer Token Filter* supports different languages, but since our corpora were all in English we used the English stemmer. Compared to the dictionary approach, algorithmic stemming needs less memory and is much faster. For this reason, it is usually better to use the *Stemmer Token Filter*. The implementation in Elasticsearch is not complicated and can be done quickly.

Irregular words or names can cause strange forms that don't give any helpful information since it is only rule based. In spite of this, we expected an improvement to the standard Elasticsearch operator. This is how we implemented the stemming analyser for Elasticsearch:

```python
stemming_analyser = {
    "filter": {
        "eng_stemmer": {
            "type": "stemmer",
            "name": "english"
        }
    },
    "analyzer": {
        "default": {
            "tokenizer": "standard",
            "filter": ["lowercase", "eng_stemmer"]
        }
    }
}
```
<a class="buttons" href="https://colab.research.google.com/github/pragmalingu/experiments/blob/master/01_Stemming/Experiment/Algorithmic_Stemmer.ipynb#scrollTo=KwsGM9oVvwIN&line=1&uniqifier=1"><button class="buttons" >Run it in Google Colab</button></a>
<br/><br/>

Afterwards, we incorporated the `stemming_analyser` into the "settings", with which we indexed the documents of each corpus:

```python
settings = {
    "settings": {
        "number_of_shards": 1,
        "number_of_replicas": 0,
        "analysis": stemming_analyser
    }
}
stemming_prefix = "stemming-"
```
<a class="buttons" href="https://colab.research.google.com/github/pragmalingu/experiments/blob/master/01_Stemming/Experiment/Algorithmic_Stemmer.ipynb#scrollTo=Fusmj_6ADAlu&line=1&uniqifier=1"><button class="buttons" >Run it in Google Colab</button></a>
<br/><br/>

These were the results with the *Stemmer Token Filter* applied:

| Stemmer Token Filter |   Cranfield |   CISI |   ADI |   Medline |   CACM |   LISA |   Time |   NPL |
|----------------------|-------------|--------|-------|-----------|--------|--------|--------|-------|
| Recall               |        0.15 |  0.121 | 0.254 |     0.032 |  0.044 |  0.394 |  0.78  | 0.285 |
| Precision            |       0.057 |  0.169 | 0.063 |     0.031 |  0.028 |  0.193 |  0.152 | 0.258 |
| F1-Score             |       0.082 |  0.141 | 0.101 |     0.032 |  0.034 |  0.259 |  0.255 | 0.271 |

Surprisingly, this had less of an impact than we had expected.

#### 2.2. Hunspell Token Filter

The *Hunspell Token Filter* from Elasticsearch recognizes irregular words and should, therefore, preprocess them better. However, we must keep in mind that words that do not appear in the dictionary are not processed correctly. In addition, a large dictionary will naturally require significantly more time and resources. Since dictionaries can never be fully complete, we didn't expect a significant change in the results compared to the *Stemmer Token Filter*. 

The procedure of the *Hunspell Token Filter* is similar to a lemmatization, but the words are not always reduced to the lemma with Hunspell. So the meaning of the words is not sufficiently addressed every time.

<details>
<summary>What is "Lemmatization"?</summary>  

Lemmatization tries to connect words to their root form - their **Lemma** - by removing all morphological rules. It's similar to stemming, but it takes affixes and plural forms into account as well, instead of simply stripping away suffixes.<br></br>
For example, `mice` would connect to `mouse`, and `froze` & `frozen` to `freeze`. 
</details>  

The procedure is the same as with the *Stemmer Token Filter*. We first implemented the Hunspell Stemming analyser:

```python
#the order of filter and analyser is arbitrary
dictionary_analyser = {
    "filter" : {
        "dictionary_stemmer" : {
          "type" : "hunspell",
          "locale" : "en_US",
          "dedup" : True  #duplicate tokens are removed from the filterâ€™s output
        }
    },
    "analyzer" : {
        "default" : {
            "tokenizer" : "standard",
            "filter" : ["lowercase", "dictionary_stemmer"]
        }
    }
}
```
<a class="buttons" href="https://colab.research.google.com/github/pragmalingu/experiments/blob/master/01_Stemming/Experiment/Hunspell_Dictionary_Stemmer.ipynb#scrollTo=_8Y8NEe5qI3c&line=1&uniqifier=1"><button class="buttons" >Run it in Google Colab</button></a>
<br/><br/>
Afterwards, we embedded the analyser into the settings to index our documents with it:

```python
settings = {
    "settings": {
        "number_of_shards": 1,
        "number_of_replicas": 0,
        "analysis": dictionary_analyser
    }
```
<a class="buttons" href="https://colab.research.google.com/github/pragmalingu/experiments/blob/master/01_Stemming/Experiment/Hunspell_Dictionary_Stemmer.ipynb#scrollTo=KwsGM9oVvwIN&line=1&uniqifier=1"><button class="buttons" >Run it in Google Colab</button></a>
<br/><br/>
These were the results of the *Hunspell Token Filter*:

| Hunspell        |   Cranfield |   CISI |   ADI |   Medline |   CACM |   LISA |   Time |   NPL |
|-----------------|-------------|--------|-------|-----------|--------|--------|--------|-------|
| Recall          |       0.149 |  0.12  | 0.256 |     0.045 |  0.037 |  0.391 |  0.777 | 0.292 |
| Precision       |       0.057 |  0.167 | 0.063 |     0.041 |  0.024 |  0.183 |  0.151 | 0.258 |
| F1-Score        |       0.082 |  0.14  | 0.101 |     0.043 |  0.029 |  0.249 |  0.252 | 0.274 |

The results were not what we had expected. We had thought that they would improve a little for every corpus, but as it turns out, the performance varies greatly from one to the next. In some cases it was even worse than the standard operator of Elasticsearch.

### 3. Results
To compare our results properly we measured Recall, Precision and F1-Score for every method on every corpus.

**Recall**
<details>
<summary>What is "Recall"?</summary>  
Recall measures the probability that relevant documents are retrieved. Therefore the number of all retrieved relevant documents is divided by the number of all documents that are labeled as relevant. For example, if we were to search 10 documents, 8 of which are relevant and 4 of these are retrieved, then the Recall measure would be 4/8 = 0.5.

To measure the Recall it is necessary to have the relevant documents labeled. Recall only looks at the documents that could be retrieved and does not take into account any irrelevant documents which may have been retrieved.

</details>  
<img alt="Recall" src={useBaseUrl('img/Recall.png')} />
<a class="buttons" href="https://colab.research.google.com/github/pragmalingu/experiments/blob/master/01_Stemming/Experiment/First_Experiment_Stemming.ipynb#scrollTo=CemuLaUAMHKP"><button class="buttons" >Run it in Google Colab</button></a>
<br/><br/>

**Precision**
<details>
<summary>What is "Precision"?</summary>  
Precision measures the probability that retrieved documents are relevant to the search query. Therefore, the number of all retrieved relevant documents is divided by the number of all retrieved documents.  For example if we retrieve 10 search results and only 5 are relevant for our search, the Precision measure would be: 5/10 = 0.5.

To measure the Precision it is necessary to have the relevant documents labeled as such. Precision only looks at the documents that are retrieved and does not account for relevant documents which were not retrieved.
</details> 
<img alt="Precision" src={useBaseUrl('img/Precision.png')} />
<a class="buttons" href="https://colab.research.google.com/github/pragmalingu/experiments/blob/master/01_Stemming/Experiment/First_Experiment_Stemming.ipynb#scrollTo=kJbezFbtMVp3"><button class="buttons" >Run it in Google Colab</button></a>
<br/><br/>
**F1-Score**
<details>
<summary>What is "F1-Score"?</summary>  
The F1-Score measures a harmonic mean between Precision and Recall. Therefore we multiply Precision and Recall by two and divide it by the sum of Precision and Recall: <br />
`F1-Score=(2*Precision*Recall)/(Precision+Recall)`
This is the simplest way to balance both Precision and Recall, there are also other common options to weight them differently.
</details> 
<img alt="F1-Score" src={useBaseUrl('img/F1-Score.png')} />
<a class="buttons" href="https://colab.research.google.com/github/pragmalingu/experiments/blob/master/01_Stemming/Experiment/First_Experiment_Stemming.ipynb#scrollTo=Gm1Aej_-T9e0"><button class="buttons" >Run it in Google Colab</button></a>
<br/><br/>

Before we started experimenting on this basic approaches, we thought we can get big improvements for the search results by simply applying stemming methods.
However, we discovered that the effect of stemming seems to vary a lot depending on the corpus.
For a more detailed analysis check out our [Simple Search vs. Stemming Comparison](./comparisons/stemming.mdx).

<a class="buttons" href="https://colab.research.google.com/github/pragmalingu/experiments/blob/master/01_Stemming/Experiment/First_Experiment_Stemming.ipynb"><button class="buttons" >Try it yourself!</button></a>