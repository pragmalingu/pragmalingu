---
id: experiment1
title: Stemming Experiment
showAuthor: true
author: Samy Ateia and Miriam Rupprecht
date: September 2020
sidebar_label: Stemming
custom_edit_url: null
---

import useBaseUrl from '@docusaurus/useBaseUrl'
import styles from '../doc.css';


For our first experiment, we created a Google Colab Notebook that runs an Elasticsearch instance and compared the standard Elasticsearch analyzer with two built-in stemming methods. 
To do this, we parsed and indexed [these 8 publically available datasets](http://ir.dcs.gla.ac.uk/resources/test_collections/), provided by the University of Glasgow, with the different analyzers 
and evaluated them with the Elasticsearch [Ranking Evaluation API](https://www.elastic.co/guide/en/elasticsearch/reference/current/search-rank-eval.html).  
[Our Parsing Guide](https://pragmalingu.de/docs/guides/how-to-parse) shows you how these datasets are structured and how we preprocessed them for indexing.
The complete code - and workflow to arrive at the results presented here - is available in our [Experiment Notebook](https://colab.research.google.com/github/pragmalingu/experiments/blob/master/01_Stemming/Experiment/First_Experiment_Stemming.ipynb) where you can reproduce this experiment step by step.

These are the two built-in methods that we compared to the [standard Elasticsearch analyzer](https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-standard-analyser.html):

* [Standard English Stemmer](https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-stemmer-tokenfilter.html) 
* [Hunspell Token Filter](https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-hunspell-tokenfilter.html) 

### 1. Standard Elasticsearch Analyzer

We ran our searches with the [multi-match query](https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-multi-match-query.html). 
It searches multiple fields of the documents - in our case 'text' and 'title' - and by default uses the highest matching score from one field to rank the document.

We evaluated the ranking on the 20 highest ranked documents returned by each query.

The following is an example request against the Ranking Evaluation API to evaluate a query on the ADI Corpus:

```python
#GET adi-corpus/_rank_eval
{
    "metric": {
        "recall": {
            "k": 20,
            "relevant_rating_threshold": 1
        }
    },
   "requests": [{
            "id": "Query_1",
            "request": {
                "query": {
                    "multi_match": {
                        "query": "How can actually pertinent data, as opposed to references or entire articles  themselves, be retrieved automatically in response to information requests?",
                        "fields": ["title", "text"]
                    }
                }
            },
            "ratings": [{
                    "_index": "adi-corpus",
                    "_id": "17",
                    "rating": 1
                }, {
                    "_index": "adi-corpus",
                    "_id": "46",
                    "rating": 1
                }, {
                    "_index": "adi-corpus",
                    "_id": "62",
                    "rating": 1
                }
            ]
        }
   ]
}
```

For our experiment we evaluated the [Precision and Recall](https://en.wikipedia.org/wiki/Precision_and_recall) at K. These are metrics provided by the Ranking Evaluation API of Elasticsearch.
The Elasticsearch documentation of the Ranking Evaluation API gives an [overview of all available metrics](https://www.elastic.co/guide/en/elasticsearch/reference/current/search-rank-eval.html#_available_evaluation_metrics).
We chose precision and recall and calculated the F1-score , which is often used for [measuring search performance](https://en.wikipedia.org/wiki/F-score#Applications) , with them.

These were the results without any further processing of the data, using the multi-match query of Elasticsearch:

|  Standard Search |   ADI |   CACM |   CISI |   Cranfield |   LISA |   Medline |   NPL |   Time |
|------------------|-------|--------|--------|-------------|--------|-----------|-------|--------|
| Recall           | 0.328 |  0.034 |  0.103 |        0.15 |  0.354 |     0.032 | 0.233 |  0.772 |
| Precision        | 0.074 |  0.022 |  0.154 |       0.056 |  0.193 |     0.031 | 0.217 |  0.15  |
| F1-Score         | 0.12  |  0.032 |  0.123 |        0.081|  0.25  |     0.027 | 0.225 |  0.251 |


### 2. Stemming

To experiment with stemming, Elasticsearch provides two built-in stemming methods. 
First is the algorithmic stemmer; it applies a series of rules to each word to reduce them to their stems. 
In Elasticsearch this is called the [Stemmer Token Filter](https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-stemmer-tokenfilter.html). 
In addition, there is the dictionary stemmer which replaces unstemmed words with stemmed variants from a provided dictionary. 
The Elasticsearch built-in method is called the [Hunspell Token Filter](https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-hunspell-tokenfilter.html). 

<details>
<summary>What is "Stemming"?</summary>  

The goal of stemming is to remove all morphological features from a word and create truncated, ambiguous **Stems**. Therefore, it tries to strip off suffixes at the end of a word.<br></br>
For example, `learning` changes to `learn` after stemming. <br></br>
If the stemmer cuts off too much information the word could become too short and lose semantic meaning; this is called overstemming.
</details>  

#### 2.1. Stemmer Token Filter

The *Stemmer Token Filter* supports different languages, but since our corpora were all in English we used the English stemmer. 
Compared to the dictionary approach, algorithmic stemming requires less memory and is significantly faster. 
For this reason, it is usually better to use the *Stemmer Token Filter*. The configuration in Elasticsearch is not complicated and can be done quickly.

Irregular words or names can cause strange forms that don't give any helpful information since the filter is only rules-based. 
In spite of this, we expected an improvement to the standard Elasticsearch operator. This is how we configured the stemming analyser in Elasticsearch:

```python
stemming_analyser = {
    "filter": {
        "eng_stemmer": {
            "type": "stemmer",
            "name": "english"
        }
    },
    "analyzer": {
        "default": {
            "tokenizer": "standard",
            "filter": ["lowercase", "eng_stemmer"]
        }
    }
}
```
<a class="buttons" href="https://colab.research.google.com/github/pragmalingu/experiments/blob/master/01_Stemming/Experiment/First_Experiment_Stemming.ipynb#scrollTo=Q9EMclEPLvMS"><button class="buttons" >Run it in Google Colab</button></a>
<br/><br/>

Afterwards, we incorporated the `stemming_analyser` into the index settings, which we then used to index the documents of each corpus:

```python
settings = {
    "settings": {
        "number_of_shards": 1,
        "number_of_replicas": 0,
        "analysis": stemming_analyser
    }
}
```
<a class="buttons" href="https://colab.research.google.com/github/pragmalingu/experiments/blob/master/01_Stemming/Experiment/First_Experiment_Stemming.ipynb#scrollTo=Q9EMclEPLvMS"><button class="buttons" >Run it in Google Colab</button></a>
<br/><br/>

These were the results with the *Stemmer Token Filter* applied:

| Stemmer Token Filter |   ADI |   CACM |   CISI |   Cranfield |   LISA |   Medline |   NPL |   Time |
|----------------------|-------|--------|--------|-------------|--------|-----------|-------|--------|
| Recall               | 0.254 |  0.044 |  0.121 |        0.15 |  0.394 |     0.032 | 0.285 |  0.78  |
| Precision            | 0.063 |  0.028 |  0.169 |       0.057 |  0.193 |     0.031 | 0.258 |  0.152 |
| F1-Score             | 0.101 |  0.034 |  0.141 |       0.082 |  0.259 |     0.032 | 0.271 |  0.255 |

#### 2.2. Hunspell Token Filter

The *Hunspell Token Filter* from Elasticsearch recognizes irregular words and should, therefore, preprocess them better. 
However, we must keep in mind that words that do not appear in the dictionary are not processed correctly. 
In addition, a large dictionary will naturally require significantly more time and resources. 
Since the dictionaries we used were not optimized for the domains of our corpora, we didn't expect a significant change in the results compared to the *Stemmer Token Filter*. 

The procedure of the *Hunspell Token Filter* is similar to a lemmatization, but the words are not always reduced to the lemma with Hunspell. 
So the meaning of the words is not sufficiently addressed every time.

<details>
<summary>What is "Lemmatization"?</summary>
Lemmatization tries to connect words to their root form - their **Lemma** - by removing all morphological rules. It's similar to stemming, but it takes affixes and plural forms into account as well, instead of simply stripping away suffixes.<br></br>
For example, `mice` would connect to `mouse`, and `froze` & `frozen` to `freeze`. 
</details>  
<br/><br/>  
The procedure is the same as with the *Stemmer Token Filter*. First, we configured the Hunspell Stemming analyser:

```python
#the order of filter and analyser is arbitrary
dictionary_analyser = {
    "filter" : {
        "dictionary_stemmer" : {
          "type" : "hunspell",
          "locale" : "en_US",
          "dedup" : True  #duplicate tokens are removed from the filterâ€™s output
        }
    },
    "analyzer" : {
        "default" : {
            "tokenizer" : "standard",
            "filter" : ["lowercase", "dictionary_stemmer"]
        }
    }
}
```
<a class="buttons" href="https://colab.research.google.com/github/pragmalingu/experiments/blob/master/01_Stemming/Experiment/First_Experiment_Stemming.ipynb#scrollTo=Y1DRyRSxZZE7"><button class="buttons" >Run it in Google Colab</button></a>
<br/><br/>
Afterwards, we embedded the analyser into the settings to index our documents with it:

```python
settings = {
    "settings": {
        "number_of_shards": 1,
        "number_of_replicas": 0,
        "analysis": dictionary_analyser
    }
```
<a class="buttons" href="https://colab.research.google.com/github/pragmalingu/experiments/blob/master/01_Stemming/Experiment/First_Experiment_Stemming.ipynb#scrollTo=Y1DRyRSxZZE7"><button class="buttons" >Run it in Google Colab</button></a>
<br/><br/>
These were the results of the *Hunspell Token Filter*:

| Hunspell        |   ADI |   CACM |   CISI |   Cranfield |   LISA |   Medline |   NPL |   Time |
|-----------------|-------|--------|--------|-------------|--------|-----------|-------|--------|
| Recall          | 0.256 |  0.037 |  0.12  |       0.149 |  0.391 |     0.045 | 0.292 |  0.777 |
| Precision       | 0.063 |  0.024 |  0.167 |       0.057 |  0.183 |     0.041 | 0.258 |  0.151 |
| F1-Score        | 0.101 |  0.029 |  0.14  |       0.082 |  0.249 |     0.043 | 0.274 |  0.252 |


### 3. Results
To compare our results properly we plotted Recall, Precision, and F1-Score for every method on every corpus.

**Recall**
<details>
<summary>What is "Recall"?</summary>  
Recall measures the probability that relevant documents are retrieved. Therefore, the number of all retrieved relevant documents is divided by the number of all documents that are labeled as relevant. For example, if we were to return 10 documents, 
8 of which are relevant to our search and 4 of these are retrieved, then the Recall measure would be 4/8 = 0.5.

To measure Recall it is necessary to have the relevant documents labeled. Recall only looks at relevant documents that were retrieved and does not take into account any irrelevant documents which may have been retrieved.
</details>  
<img alt="Recall" src={useBaseUrl('img/EXP1_Recall.png')} />
<a class="buttons" href="https://colab.research.google.com/github/pragmalingu/experiments/blob/master/01_Stemming/Experiment/First_Experiment_Stemming.ipynb#scrollTo=CemuLaUAMHKP"><button class="buttons" >Run it in Google Colab</button></a>
<br/><br/>

**Precision**
<details>
<summary>What is "Precision"?</summary>  
Precision measures the probability that retrieved documents are relevant to the search query. Therefore, the number of all retrieved relevant documents is divided by the number of all retrieved documents.  
For example if we retrieve 10 search results and only 5 are relevant for our search, the Precision measure would be: 5/10 = 0.5.

To measure the Precision it is necessary to have the relevant documents labeled as such. Precision only looks at the documents that are retrieved and does not account for relevant documents which were not retrieved.
</details> 
<img alt="Precision" src={useBaseUrl('img/EXP1_Precision.png')} />
<a class="buttons" href="https://colab.research.google.com/github/pragmalingu/experiments/blob/master/01_Stemming/Experiment/First_Experiment_Stemming.ipynb#scrollTo=kJbezFbtMVp3"><button class="buttons" >Run it in Google Colab</button></a>
<br/><br/>  

**F1-Score**
<details>
<summary>What is "F1-Score"?</summary>  
The F1-Score measures a harmonic mean between Precision and Recall. Therefore we multiply Precision and Recall by two and divide it by the sum of Precision and Recall: <br />
`F1-Score=(2*Precision*Recall)/(Precision+Recall)`
This is the simplest way to balance both Precision and Recall, there are also other common options to weight them differently.
</details> 
<img alt="F1-Score" src={useBaseUrl('img/EXP1_F1-Score.png')} />
<a class="buttons" href="https://colab.research.google.com/github/pragmalingu/experiments/blob/master/01_Stemming/Experiment/First_Experiment_Stemming.ipynb#scrollTo=Gm1Aej_-T9e0"><button class="buttons" >Run it in Google Colab</button></a>
<br/><br/>


It is apparent that the algorithmic stemmer offers some improvements over no stemming. The dictionary based hunspell stemming seems to perform either better or worse depending on the corpora.
For a more detailed analysis check out our [Simple Search vs. Stemming Comparison](./comparisons/stemming.mdx).

<a class="buttons" href="https://colab.research.google.com/github/pragmalingu/experiments/blob/master/01_Stemming/Experiment/First_Experiment_Stemming.ipynb"><button class="buttons" >Try it yourself!</button></a>

<br/>
<br/>


**Acknowledgements:**<br/>
Thanks to Dario Alves for implementing the Stemmers inside of Google Colab for Elasticsearch and Kenny Hall for proofreading this article.

<div className="col text--right">
    <em>
        <small>
            Written by <strong>Samy Atei</strong> and <strong>Miriam Rupprecht</strong>,  September 2020
        </small>
    </em>
</div>