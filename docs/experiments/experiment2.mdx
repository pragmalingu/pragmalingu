---
id: experiment2
title: Sentence Embeddings Experiment
sidebar_label: Sentence Embeddings
custom_edit_url: null
---
import useBaseUrl from '@docusaurus/useBaseUrl'
import styles from '../doc.css';

For our second experiment we connected a Colab Notebook to an Elasticsearch instance and compared the standard Elasticsearch operator with our first embedding approach. To do this,  we first transformed the documents into vector representations by using a BERT model and indexed them. Next, we searched in the transformed documents with a vector representation of the query.
Since the BERT model is designed to work on sentences, we searched both the document titles, as well as the entire text, and compared the results.
 
We have already described how the standard Elasticsearch operator works and how we implemented it in [our first experiment](./experiment1.mdx).


<details>
<summary>Need a short summary of the Multi-Match Query?</summary>  

The standard Elasticsearch method - [Multi-Match Query](https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-simple-query-string-query.html) - is a full-text search which includes standard tokenization as well as an analyzer. It searches only the given fields of the documents; in our case 'text' and 'title'. To gather our data, we only looked at the first 20 retrieved documents for any given search.

</details>  

### 1. What Is BERT?

BERT is short for Bi-directional Encoder Representation from Transformer and is an approach which uses large, pre-trained, neural networks with some exceptional solutions to get vector representations (embeddings) from texts. Using these embeddings we can use similarity metrics - such as cosine similarity - to compare the meaning of the texts.
 
(By the way, these networks are frequently used as a backbone or part of models to solve some NLP tasks like Question Answering, Ranking, Named Entity Recognition, etc.)
 
["I'm brave enough to read the paper on BERT"](https://arxiv.org/abs/1810.04805)
 
We also try to explain BERT in our [Guide About Transformers and Embeddings.](./guides/embeddings-transformers.mdx)


### 2. Preparing The BERT Model 

For our experiment, we decided on a BERT model for sentence embeddings via the [Hugging Face](https://huggingface.co/) website. The selected pre-trained BERT model was then used to convert the desired search fields from the documents and the search query into embeddings. To get comparable values we ranked the search results based on the cosine similarity between the sentence embeddings and query embeddings.

<details>
<summary>What is cosine similarity?</summary>  

The cosine similarity is used to measure the difference between two vectors irrespective of their size. In NLP they often represent words, sentences, or even whole documents. To get the cosine similarity the angle between two vectors - which are projected in a multi-dimensional space - is calculated. The smaller the angle, the higher the cosine similarity. Therefore, if the cosine similarity is 1, the vectors are identical. Read more details on our [Basic Definition](../guides/basic-definitions.mdx) page.

</details> 

<br></br>

We then tried to improve the results by re-ranking the top 100 results of a normal multi-match query using the cosine similarity.

**Initialize A Pre-trained BERT Model**

Before we could start experimenting, we first had to choose a pre-trained model from the BERT library, download it, and initialize it. Our first choice was the BERT model ["bert-base-nli-mean-tokens"](https://www.sbert.net/docs/pretrained_models.html#semantic-textual-similarity). This model can also be found in the [sentence-transformers-repository](https://github.com/UKPLab/sentence-transformers), it was developed to generate sentence embeddings.
(For more information on this model you can read the paper [Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks](https://arxiv.org/abs/1908.10084))

To initialize the model we ran the following code:
 
```python
pip install -U sentence-transformers

from sentence_transformers import SentenceTransformer

model = SentenceTransformer('bert-base-nli-mean-tokens')

```
<a class="buttons" href="https://colab.research.google.com/github/pragmalingu/experiments/blob/master/02_Embeddings/Experiment/BERT_Embeddings_example.ipynb"><button class="buttons" >Run BERT model in Colab</button></a>

### 3. Experimenting With Sentence Embeddings From BERT 
After initializing the BERT model, we converted the documents and the search queries into sentence embeddings. Then - similarly to our last experiment - we calculated comparable values using the [Ranking Evaluation API](../guides/ranking-api.mdx).

#### 3.1. Preparing The Documents
Now that the model had been integrated into our Notebook, we could start transforming our texts with BERT. To be able to compare the documents and the search query using cosine similarity, we first had to convert the desired search fields into embeddings and feed them into Elasticsearch.
Since we worked with the Elasticsearch param `dense_vector`, it is not possible to recreate this experiment on Open Distro for Elasticsearch, it is necessary to use the standard Elasticsearch installation instead.

**Title Embeddings**<br></br>
For our first approach, we tried to calculate the similarity between the search queries and the titles of the documents. To do this, we set `title_vector` as a container for the computed BERT embedding in addition to the field `title`, which should be the container for the text of the title:

```json
# Settings for BERT title search
settings_title = {
  "mappings": {
      "properties": {
          "title": {
              "type": "text"
              },
          "title_vector": {
           "type": "dense_vector",
           "dims": 768
            }
    }
  }
}
```

While indexing, the code iterates over the preprocessed corpus and encodes the title for each document using the BERT model:

```python
for ID, doc_data in adi_txt_data.items():
# index for BERT title search
    es.index(
      index=adi_index_title, 
      id=ID, 
      body={
          'title_vector': model.encode(adi_txt_data[ID]['title']),
          'title': adi_txt_data[ID]['title'],
          }
    )
```
<a class="buttons" href="https://colab.research.google.com/github/pragmalingu/experiments/blob/master/02_Embeddings/Experiment/Second_Experiment_BERT_Embeddings.ipynb#scrollTo=ZaxpCqSDShgg&line=1&uniqifier=1"><button class="buttons" >Run this in Colab</button></a>

*Note*: Not every corpus has titles, that is why we excluded the `Medline`, `NPL` and `Time` corpus for the title search.

| BERT on title field  |   ADI |   CACM |   CISI |   Cranfield |   LISA |   Medline |   NPL |   Time |
|----------------------|-------|--------|--------|-------------|--------|-----------|-------|--------|
| Recall               | 0.227 |  0.007 |  0.038 |       0.071 |  0.139 |     0     | 0     |  0     |
| Precision            | 0.227 |  0.006 |  0.088 |       0.028 |  0.056 |     0     | 0     |  0     |
| F-1-Score            | 0.227 |  0.006 |  0.054 |       0.04  |  0.08  |     0     | 0     |  0     |

**Text Embeddings**<br></br>
The procedure for working with the whole text was a bit more complicated as we first had to split the text into sentences. Otherwise the embeddings would have been far too long or  even cut off. Therefore, we created a new category inside the corpus dictionaries to store the split sentences for easy access:

```python
# transform text to sentences for BERT text search

import nltk
from nltk import tokenize
from nltk.tokenize import sent_tokenize
nltk.download('punkt')

def text_to_sentences(string):
  sentences = tokenize.sent_tokenize(string)
  return sentences

for ID, doc in adi_txt_data.items():
  text = adi_txt_data[ID]['text']
  adi_txt_data[ID]['sentences'] = text_to_sentences(text)
```

Since a text is represented by more than one sentence, we needed to create a container to store all of the sentence embeddings. Therefore, we initialized `text_vector` as seen below, which is a nested container. The whole, unsplit text is stored in the container `text`:

```json
# Settings for BERT text search
settings_text = {
  "mappings": {
    "properties": {
      "text_vector": {
        "type": "nested",
        "properties": {
          "vector": {
            "type": "dense_vector",
            "dims": 768 
          }
        }
      },
     "text": {
        "type": "keyword"
    },
    }
  }
}
```

To keep the syntax of Elasticsearch intact, it was necessary to store the sentence embeddings into a list before giving it to the index field:

```python
for ID, doc_data in adi_txt_data.items():
# index for BERT text search
     es.index(
      index=adi_index_text, 
      id=ID, 
      body={
          'text_vector': [{"vector": model.encode(text)} for text in adi_txt_data[ID]['sentences']],
          'text': adi_txt_data[ID]['text']
          }
    )
```

<a class="buttons" href="https://colab.research.google.com/github/pragmalingu/experiments/blob/master/02_Embeddings/Experiment/Second_Experiment_BERT_Embeddings.ipynb#scrollTo=ZaxpCqSDShgg&line=1&uniqifier=1"><button class="buttons" >Run this in Colab</button></a>

*Note*: Not every document has text, so for some corpora we needed to check that while indexing, e.g. the `CACM` corpus.

| BERT on text field   |   ADI |   CACM |   CISI |   Cranfield |   LISA |   Medline |   NPL |   Time |
|----------------------|-------|--------|--------|-------------|--------|-----------|-------|--------|
| Recall               | 0.204 |  0.018 |  0.024 |       0.039 |  0.055 |     0.027 | 0.099 |  0.156 |
| Precision            | 0.059 |  0.012 |  0.053 |       0.012 |  0.031 |     0.031 | 0.089 |  0.019 |
| F-1-Score            | 0.091 |  0.014 |  0.033 |       0.019 |  0.04  |     0.029 | 0.094 |  0.034 |

#### 3.2 Search With Cosine Similarity
Once we had prepared all of the documents, the only thing left was  to compute the search queries before we could pass them to the Ranking Evaluation API. For the search on the title fields a `script_score` request for each query was enough to get results. Our `script_score` calculates the cosine similarity between each document title and the search query before returning the top 20 results:

```python
"query" : {
  "script_score": {
    "query": {"match_all": {}},
    "script": {
      "source": "cosineSimilarity(params.query_vector, doc['title_vector']) + 1.0",
      "params": {"query_vector": list(model.encode(query_txt['question']).astype(float))}
    }
  }
  }
    }
```
For the search on the `text_vector` embeddings it was a bit more complicated. Since the embeddings were saved as a list, we had to create a nested query which calculates the cosine similarity between each sentence and the query. The best cosine similarity value becomes the value for the entire document:

```python
"query": {
            "nested": {
                "path": "text_vector",
                "score_mode": "min", 
                "query": {
                    "function_score": {
                        "script_score": {
                            "script": {
                                "source": "1.0 + cosineSimilarity(params.query_vector, 'text_vector.vector')",
                                "params": {"query_vector": list(model.encode(query_txt['question']).astype(float))}
                                }
                                }
                                }
                          }
                       }
                  }
                  }
```

#### 3.3. Search with Re-ranking 
Since the text embeddings doesn't seem to be working as good as the standard Elasticsearch operator, we decided to try a re-ranking on the documents. Re-ranking is the act of ranking something differently. In Elasticsearch the operator to achieve this is called [Rescoring'](https://www.elastic.co/guide/en/elasticsearch/reference/6.8/search-request-rescore.html). 

Therefore, we run a normal multi-match query on the given field ("title" or "text") and rescore the top 100 results from that query with our cosine similarity BERT search.

In this example we use our search on the "text"-field and the cosine similarity search on the sentence embeddings:
```python
{
  "query": {
    "multi_match" : {
                  "query" : query_txt['question'],
                  "fields" : ['text']
            }
  },
  "rescore" : {
      "window_size" : 100,
      "query": {
          "rescore_query" : {
            "nested": {
                "path": "text_vector",
                "score_mode": "min", 
                "query": {
                    "function_score": {
                        "script_score": {
                            "script": {
                                "source": "1.0 + cosineSimilarity(params.query_vector, 'text_vector.vector')",
                                "params": {"query_vector": list(model.encode(query_txt['question']).astype(float))}
                                }
                                }
                                }
                          }
                       }
            }
                  }
}
    }
```


| Re-Ranking on title field |   ADI |   CACM |   CISI | Cranfield |  LISA |
|---------------------------|-------|--------|--------|-----------|-------|
| Recall                    | 0.266 |  0.029 |  0.059 |     0.118 | 0.293 |
| Precision                 | 0.059 |  0.017 |  0.094 |     0.044 | 0.141 |
| F1-Score                  | 0.096 |  0.022 |  0.072 |     0.064 | 0.191 |


| Re-ranking on text field |   ADI |   CACM |   CISI | Cranfield |  LISA |   Medline |   NPL |   Time |
|--------------------------|-------|--------|--------|-----------|-------|-----------|-------|--------|
| Recall                   | 0.303 |  0.035 |  0.103 |     0.156 | 0.313 |     0.032 | 0.233 |  0.772 |
| Precision                | 0.075 |  0.023 |  0.154 |     0.058 | 0.14  |     0.031 | 0.218 |  0.15  |
| F1-Score                 | 0.12  |  0.028 |  0.124 |     0.085 | 0.193 |     0.032 | 0.225 |  0.251 |


#### 3.4. Combining those Approaches
To get even better results we then tried to combine all the previous approaches into a more complex Elasticsearch query.
Therefore, we searched on both - "title" and "text"-field - and reranked afterwards using the cosine similarity of the title embedding and the sentence embeddings of the text. Since it's more likely so have a meaningful title we boosted the value of the cosine similarity for the title embeddings. The boosting can be adjusted by changing the values of the variables 'query_weight' and 'rescore_query_weight' using a float().

This is the code we used:
```python
{
  "query": {
    "multi_match" : {
                  "query" : query_txt['question'],
                  "fields" : ['text','title']
            }
  },
  "rescore" :  [
                {
      "window_size" : 100,
      "query" : {
         "rescore_query" : {
              "script_score": {
    "query": {"match_all": {}},
    "script": {
      "source": "cosineSimilarity(params.query_vector, doc['title_vector']) + 1.0",
      "params": {"query_vector": list(model.encode(query_txt['question']).astype(float))}
    }
            }
      },
         "query_weight" : query_weight,
         "rescore_query_weight" : rescore_query_weight
   }
},
                {
      "window_size" : 100,
      "query": {
          "rescore_query" : {
            "nested": {
                "path": "text_vector",
                "score_mode": "min", 
                "query": {
                    "function_score": {
                        "script_score": {
                            "script": {
                                "source": "1.0 + cosineSimilarity(params.query_vector, 'text_vector.vector')",
                                "params": {"query_vector": list(model.encode(query_txt['question']).astype(float))}
                                }
                                }
                                }
                          }
                       }
            }
                  }
}  
  ]}
```

| Complex re-ranking query |   ADI |   CACM |   CISI | Cranfield |  LISA |
|--------------------------|-------|--------|--------|-----------|-------|
| Recall                   | 0.288 |  0.035 |  0.105 |     0.159 | 0.37  |
| Precision                | 0.065 |  0.023 |  0.158 |     0.058 | 0.167 |
| F1-Score                 | 0.106 |  0.028 |  0.126 |     0.085 | 0.23  |


### 4. Results

In order to get suitable comparison values, we decided to carry out the multi-match query in the same way as the BERT search on the title or text field. In addition to that we tried out how effective re-ranking can be on the BERT results and also build a more complex query for Elasticsearch to combine everything we evaluated before.
To compare our results properly we measured Recall, Precision and F1-Score for every method on every corpus. We divided the results into the search on "title"-field,  on "text"-field and with our more complex query, to avoid confusion.

<details>
<summary>What is "Recall"?</summary>  
Recall measures the probability that relevant documents are retrieved. Therefore, the number of all retrieved relevant documents is divided by the number of all documents that are labeled as relevant. For example, if we were to search 10 documents, 8 of which are relevant and 4 of these are retrieved, then the Recall measure would be 4/8 = 0.5.
 
To measure the Recall it is necessary to have the relevant documents labeled. Recall only looks at the documents that could be retrieved and does not take into account any irrelevant documents which may have been retrieved.

</details>  

<details>
<summary>What is "Precision"?</summary>  
Precision measures the probability that retrieved documents are relevant to the search query. Therefore, the number of all retrieved relevant documents is divided by the number of all retrieved documents.  For example, if we retrieve 10 search results and only 5 are relevant for our search, then the Precision measure would be: 5/10 = 0.5.
 
To measure the Precision it is necessary to have the relevant documents labeled as such. Precision only looks at the documents that are retrieved and does not account for relevant documents which were not retrieved.
</details> 

<details>
<summary>What is "F1-Score"?</summary>  
The F1-Score measures a harmonic mean between Precision and Recall. Therefore, we multiply Precision and Recall by two and divide it by the sum of Precision and Recall: <br />
`F1-Score=(2*Precision*Recall)/(Precision+Recall)`
This is the simplest way to balance both Precision and Recall, there are also other common options to weight them differently.
</details> 

#### 4.1. Search on the title field
These are the results for the search on the "title"-field  which compare the multi-match query, a BERT search and a re-ranking on the top 100 results using BERT. Since the data sets `Medline`,` NPL` and `Time` do not have any titles, the title search was left out for these corpora as with the last experiment.

**Recall**
<img alt="Recall on title field" src={useBaseUrl('img/EXP2_Recall_title.png')} />
<a class="buttons" href="https://colab.research.google.com/github/pragmalingu/experiments/blob/master/02_Embeddings/Experiment/Second_Experiment_BERT_Embeddings.ipynb#scrollTo=crBSdbZMPAH9&line=1&uniqifier=1"><button class="buttons" >Run this in Google Colab</button></a>
<br/><br/>

**Precision**
<img alt="Precision on title" src={useBaseUrl('img/EXP2_Precision_title.png')} />
<a class="buttons" href="https://colab.research.google.com/github/pragmalingu/experiments/blob/master/02_Embeddings/Experiment/Second_Experiment_BERT_Embeddings.ipynb#scrollTo=Bp4g4N7TUpXB&line=1&uniqifier=1"><button class="buttons" >Run this in Google Colab</button></a>
<br /><br />

**F1-Score**
<img alt="F1-Score on title field" src={useBaseUrl('img/EXP2_Fscore_title.png')} />
<a class="buttons" href="https://colab.research.google.com/github/pragmalingu/experiments/blob/master/02_Embeddings/Experiment/Second_Experiment_BERT_Embeddings.ipynb#scrollTo=mYfPAzYTVwRQ&line=1&uniqifier=1"><button class="buttons" >Run this in Google Colab</button></a>
<br/><br/>


#### 4.2. Search on the text field
These are the results for the search on the "text"-field which compare the multi-match query, a BERT search and a re-ranking on the top 100 results using BERT.

**Recall**
<img alt="Recall on text field" src={useBaseUrl('img/EXP2_Recall_text.png')} />
<a class="buttons" href="https://colab.research.google.com/github/pragmalingu/experiments/blob/master/02_Embeddings/Experiment/Second_Experiment_BERT_Embeddings.ipynb#scrollTo=BXibzPzmf_KK&line=1&uniqifier=1"><button class="buttons" >Run this in Google Colab</button></a>
<br/><br/>

**Precision**
<img alt="Precision on text" src={useBaseUrl('img/EXP2_Precision_text.png')} />
<a class="buttons" href="https://colab.research.google.com/github/pragmalingu/experiments/blob/master/02_Embeddings/Experiment/Second_Experiment_BERT_Embeddings.ipynb#scrollTo=tRF25Yq3G0QU&line=1&uniqifier=1"><button class="buttons" >Run this in Google Colab</button></a>
<br /><br />

**F1-Score**
<img alt="F1-Score on text field" src={useBaseUrl('img/EXP2_Fscore_text.png')} />
<a class="buttons" href="https://colab.research.google.com/github/pragmalingu/experiments/blob/master/02_Embeddings/Experiment/Second_Experiment_BERT_Embeddings.ipynb#scrollTo=QA8CpbbRQtIH&line=1&uniqifier=1"><button class="buttons" >Run this in Google Colab</button></a>
<br/><br/>

#### 4.3. Search with a complex re-ranking query
These are the results for the search with a more complex query which searches on both fields "title" and "text" and reranks these results afterwards with BERT on the title and on the text. The title field is boosted while reranking. To see the difference these results are compared with the multi-match query on the "title"- and "text"-field, as with the BERT search on "title" and "text".

**Recall**
<img alt="Recall on complex re-ranking query" src={useBaseUrl('img/EXP2_Recall_complex.png')} />
<a class="buttons" href="https://colab.research.google.com/github/pragmalingu/experiments/blob/master/02_Embeddings/Experiment/Second_Experiment_BERT_Embeddings.ipynb#scrollTo=tJBsXlxhmgPV&line=1&uniqifier=1"><button class="buttons" >Run this in Google Colab</button></a>
<br/><br/>

**Precision**
<img alt="Precision on complex re-ranking query" src={useBaseUrl('img/EXP2_Precision_complex.png')} />
<a class="buttons" href="https://colab.research.google.com/github/pragmalingu/experiments/blob/master/02_Embeddings/Experiment/Second_Experiment_BERT_Embeddings.ipynb#scrollTo=N41wwBZA1Z3v&line=1&uniqifier=1"><button class="buttons" >Run this in Google Colab</button></a>
<br /><br />

**F1-Score**
<img alt="F1-Score on complex re-ranking query" src={useBaseUrl('img/EXP2_Fscore_complex.png')} />
<a class="buttons" href="https://colab.research.google.com/github/pragmalingu/experiments/blob/master/02_Embeddings/Experiment/Second_Experiment_BERT_Embeddings.ipynb#scrollTo=hO3oqgZE2ys2&line=1&uniqifier=1"><button class="buttons" >Run this in Google Colab</button></a>
<br/><br/>


For a more detailed analysis check out our [Embedding Comparison](./comparisons/embeddings.mdx).

<a class="buttons" href="https://colab.research.google.com/github/pragmalingu/experiments/blob/master/02_Embeddings/Experiment/Second_Experiment_BERT_Embeddings.ipynb"><button class="buttons" >Try it yourself!</button></a>


<br/>
<br/>

**Acknowledgements:**<br/>
Thanks to Pavel Prokopev for the pre-work on the BERT Embeddings and Kenny Hall for proofreading this article.

<div className="col text--right">
    <em>
        <small>
            Written by <strong>Miriam Rupprecht</strong>,  December 2020
        </small>
    </em>
</div>