---
id: experiment2
title: Sentence Embeddings Experiment
sidebar_label: Sentence Embeddings
custom_edit_url: null
---

For our second experiment we connect the Notebook to an Elasticsearch instance and compared a standard Elasticsearch operator with our BERT approach. Therefore we got the vector representations for the documents from BERT and indexed them using an Elasticsearch built-in knn algorithm. 
We tried this approach in the whole document texts as well as on the document titles.

We described how the standard Elasticsearch operator works and how we implemented it in [our first experiment](./experiment1.mdx).

<details>
<summary>Need a short summary?</summary>  

We implemented our search with a standard Elasticsearch method - [multi-match query](https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-simple-query-string-query.html) - which is a full-text search that comes with standard tokenization and an analyser. It searches only the given fields of the documents - in our case 'text' and 'title'. To gather our data, we looked only at the first 20 retrieved documents for any given search.

</details>  

### 1. What is BERT and how does it work?

Bi-Directional Encoder Representation from Transformer (BERT) is an approach of using large pre-trained neural networks with some exceptional solutions to get the vectors from texts, which we can use with some similarity metrics like cosine similarity to compare meaning of these texts.

(By the way these networks are frequently used as a backbone or part of ensemble of models to solve some NLP tasks like Question Answering, Ranking, Named Entity Recognition, etc.)

["I'm brave enough to read the paper on BERT"](https://arxiv.org/abs/1810.04805)

We also try to explain BERT in our [guide on Transformer and Embeddings.](./guides/embeddings-transformers.mdx)

### 2. Experimenting With Sentence Embeddings From BERT 

Für unser Experiment haben wir uns mittels der [huggingface](https://huggingface.co/) website für ein BERT model entschieden, um damit erste Experimente laufen zu lassen. Das ausgewählte pre-trained BERT model haben wir  benutzt, um die gewünschten Suchfelder, sowie die Suchanfrage in Embeddings umzuwandeln. Die Suchanfragen haben wir anhand der Cosinusähnlichkeit gerankt und so versucht bessere Ergebnisse als in der normalen Multi Match Query zu erzielen.

#### 2.1. Intializie Pretrained BERT Model
Bevor wir anfangen konnten zu experimentieren mussten wir uns zunächst ein pretrained model aus der BERT library aussuchen, dieses herunterladen und intialisieren. Wir haben uns hierbei erstmal für das BERT-model 'bert-base-nli-mean-tokens' entschieden. Dieses Model stammt aus dem [sentence-transformers-repository](https://github.com/UKPLab/sentence-transformers) und wurde entwickelt um damit sentence embeddings zu generieren. 
(For more information on this model you can read the paper [Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks](https://arxiv.org/abs/1908.10084))
 
```python
pip install -U sentence-transformers

from sentence_transformers import SentenceTransformer
import torch
from tqdm import tqdm_notebook

model = SentenceTransformer('bert-base-nli-mean-tokens')

# using gpu to boost inference if it's possible
if torch.cuda.is_available():
  model.to('cuda')

```

#### 2.2. Preparing The Title Field

```json
# Settings for BERT title search
settings_title = {
  "mappings": {
      "properties": {
          "title": {
              "type": "text"
              },
          "title_vector": {
           "type": "dense_vector",
           "dims": 768
            }
    }
  }
}
```

```python
# index for BERT title search
    es.index(
      index=adi_index_title, 
      id=ID, 
      body={
          'title_vector': model.encode(adi_txt_data[ID]['title']),
          'title': adi_txt_data[ID]['title'],
          }
    )
```



#### 2.3. Preparing The Text Field

```python
import nltk
from nltk import tokenize
from nltk.tokenize import sent_tokenize
nltk.download('punkt')

def text_to_sentences(string):
  sentences = tokenize.sent_tokenize(string)
  return sentences

for ID, doc in cacm_txt_data.items():
  if 'text' in cacm_txt_data[str(ID)]:
    text = cacm_txt_data[ID]['text']
    cacm_txt_data[ID]['sentences'] = text_to_sentences(text)
```

```json
# Settings for BERT text search
settings_text = {
  "mappings": {
    "properties": {
      "text_vector": {
        "type": "nested",
        "properties": {
          "vector": {
            "type": "dense_vector",
            "dims": 768 
          }
        }
      },
     "text": {
        "type": "keyword"
    },
    }
  }
}
```

```python
# index for BERT text search
    text_list = []
    for text in adi_txt_data[ID]['sentences']:
      text_list.append({"vector": model.encode(text)})
    es.index(
      index=adi_index_text, 
      id=ID, 
      body={
          'text_vector': text_list,
          'text': adi_txt_data[ID]['text']
          }
    )
```

### 3. Search With Cosine Similarity

```python
"query" : {
  "script_score": {
    "query": {"match_all": {}},
    "script": {
      "source": "cosineSimilarity(params.query_vector, doc['title_vector']) + 1.0",
      "params": {"query_vector": list(model.encode(query_txt['question']).astype(float))}
    }
  }
  }
    }
```

nested query for the sentences search

```python
"query": {
            "nested": {
                "path": "text_vector",
                "score_mode": "min", 
                "query": {
                    "function_score": {
                        "script_score": {
                            "script": {
                                "source": "1.0 + cosineSimilarity(params.query_vector, 'text_vector.vector')",
                                "params": {"query_vector": list(model.encode(query_txt['question']).astype(float))}
                                }
                                }
                                }
                          }
                       }
                  }
                  }
```

### 4. Results

To compare our results properly we measured Recall, Precision and F1-Score for every method on every corpus.

**Recall**
<details>
<summary>What is "Recall"?</summary>  
Recall measures the probability that relevant documents are retrieved. Therefore the number of all retrieved relevant documents is divided by the number of all documents that are labeled as relevant. For example, if we were to search 10 documents, 8 of which are relevant and 4 of these are retrieved, then the Recall measure would be 4/8 = 0.5.

To measure the Recall it is necessary to have the relevant documents labeled. Recall only looks at the documents that could be retrieved and does not take into account any irrelevant documents which may have been retrieved.

</details>  
<img alt="Recall" src={useBaseUrl('img/EXP2_Recall.png')} />
<a class="buttons" href="https://colab.research.google.com/drive/1UW6YZUYIYHBWUweTxghk9eCXWp4GbObl#scrollTo=CemuLaUAMHKP"><button class="buttons" >Run this in Google Colab</button></a>
<br/><br/>

**Precision**
<details>
<summary>What is "Precision"?</summary>  
Precision measures the probability that retrieved documents are relevant to the search query. Therefore, the number of all retrieved relevant documents is divided by the number of all retrieved documents.  For example if we retrieve 10 search results and only 5 are relevant for our search, the Precision measure would be: 5/10 = 0.5.

To measure the Precision it is necessary to have the relevant documents labeled as such. Precision only looks at the documents that are retrieved and does not account for relevant documents which were not retrieved.
</details> 
<img alt="Precision" src={useBaseUrl('img/EXP2_Precision.png')} />
<a class="buttons" href="https://colab.research.google.com/drive/1UW6YZUYIYHBWUweTxghk9eCXWp4GbObl#scrollTo=kuHz0wJvFBzW"><button class="buttons" >Run this in Google Colab</button></a>
<br /><br />

**F1-Score**
<details>
<summary>What is "F1-Score"?</summary>  
The F1-Score measures a harmonic mean between Precision and Recall. Therefore we multiply Precision and Recall by two and divide it by the sum of Precision and Recall: <br />
`F1-Score=(2*Precision*Recall)/(Precision+Recall)`
This is the simplest way to balance both Precision and Recall, there are also other common options to weight them differently.
</details> 
<img alt="F1-Score" src={useBaseUrl('EXP2_F1-Score.png')} />
<a class="buttons" href="https://colab.research.google.com/drive/1UW6YZUYIYHBWUweTxghk9eCXWp4GbObl#scrollTo=Gm1Aej_-T9e0"><button class="buttons" >Run this in Google Colab</button></a>
<br/><br/>

Before we started experimenting on this basic approaches, we thought we can get big improvements for the search results by simply applying stemming methods.
However, we discovered that the effect of stemming seems to vary a lot depending on the corpus.
For a more detailed analysis check out our [Simple Search vs. Stemming Comparison](./comparisons/stemming.mdx).

<a class="buttons" href="https://colab.research.google.com/drive/1UW6YZUYIYHBWUweTxghk9eCXWp4GbObl#scrollTo=gpKrbKC7TDxu"><button class="buttons" >Try it yourself!</button></a>