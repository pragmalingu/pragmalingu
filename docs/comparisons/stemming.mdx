---
id: stemming
title: Simple Search vs. Stemming
author: Samy Ateia and Miriam Rupprecht
date: April 2021
sidebar_label: Stemming
custom_edit_url: null
---
import useBaseUrl from '@docusaurus/useBaseUrl'
import styles from '../doc.css';

## 1. What is Stemming?
Stemming is the process of removing the alternating endings from words.  
The words **fish**, **fishing**, **fished** would all be reduced to their common word stem **fish**. 
This stem is then stored in the inverted index of the search engine instead of the original word at that position. 
When you search with either of the original words, your query is also stemmed and the search engine can find all documents that contain words with the same stem.  

There are two common approaches to implement stemming in search engines.  


### Algorithmic Stemming
The [Porter stemming alogrithm](https://snowballstem.org/algorithms/porter/stemmer.html) created by [Martin Porter](https://en.wikipedia.org/wiki/Martin_Porter) is one of the most popular algorithmic stemmers 
and used by default in the built in [language analyzers](https://www.elastic.co/guide/en/elasticsearch/reference/7.12/analysis-stemmer-tokenfilter.html) in Elasticsearch.
Algorithmic stemmers are very fast and don't need alot of memory, because they only apply a set of rules, that represent the common rules how words are [inflected](https://en.wikipedia.org/wiki/Inflection) in a specific language.  
Because there are always exeptions to a rule, an algorithmic stemmer might produce the same stem for words that have different meanings.
This error is called **overstemming**. 
Another error that can happen is called **understemming** and happens when two inflected words with different endings but the same meaning and stem are not reduced to the same stem.

### Dictionary Based Stemming
The second common approach to stemming is [dictionary based stemming](https://www.elastic.co/guide/en/elasticsearch/reference/current/stemming.html#dictionary-stemmers).
Dictionary stemmers look up words in a dictionary before they remove their inflected endings and reduce them to their stem.
Dictionary stemmers should work better on iregular words or words that are similar but have different meanings.
They also should be able to reduce over and understemming and lead to better results than algorithmic stemmers.
In practice they sometimes perform worse than algorithmic stemmers, because freely available dictionaries are missing important words or are outdated.
Dictionary stemmers also have the draw back that their are slower and require more memory because they have to lookup all words in their dictionary.

### Comparison
In this comparison we are answering the following questions:
- How much does the relevancy of search results improve by using a stemmer?
- How good is the hunspell stemmer that is included in Elasticsearch performing with a free dictionary?

## 2. Results
The following are the results from our [first Experiment](/docs/experiments/experiment1). You can reproduce these results yourself if you follow the instructions on our [experiment section](/docs/experiments/).

**F1-Score**
<details>
<summary>What is "F1-Score"?</summary>  
The F1-Score measures a harmonic mean between Precision and Recall. Therefore we multiply Precision and Recall by two and divide it by the sum of Precision and Recall: <br />
`F1-Score=(2*Precision*Recall)/(Precision+Recall)`
This is the simplest way to balance both Precision and Recall, there are also other common options to weight them differently.
</details> 


<img alt="F1-Score" src={useBaseUrl('img/EXP1_F1-Score.png')} />
<a class="buttons" href="https://colab.research.google.com/github/pragmalingu/experiments/blob/master/01_Stemming/Evaluation/Comparison_Stemming_vs_Standard_Search.ipynb#scrollTo=F_WKHLDXJypJ"><button class="buttons" >Reproduce it in Google Colab</button></a>
<br />
<br />


On all evaluation datasets except NPL, the algorithmic stemmer outperformed the standard analyzer as well as the hunspell token filter.
The achieved F1-Score gains over the standard analyzer in percentage points can be seen here:

<img alt="F1-Score Gains" src={useBaseUrl('img/EXP1_Gains.svg')} />
<br /><br />
Depending on the use-case algorithmic stemming can lead up to a 22% improvement in search result relevancy.
Hunspell stemming performs worse than algorithmic stemming on nearly all datasets and can even lead to worse results than no stemming.

The average F1-Score gains over all datasets are:

<img alt="F1-Score Gains" src={useBaseUrl('img/EXP1_Average.svg')} />

Using the stemmer token filter on english documents can therefore improve the f1 score by 11% on average.

## 3. Discussion

Looking at the f1 score fom our approaches we can see that both stemming approaches perform better than the standard baseline on most datasets.
Algorithmic stemming performs better than the standard analyzer on all datasets, it has the biggest gains on the CACM and NPL datasets.
Hunspell stemming performs worse than algorithmic stemming on most datasets except NPL where it performs slighlty better.
Hunspell stemming performs worse than the standard analyzer on the medline dataset.

The are a list of interesting questions that we can ask by just looking at these results:

1.   Why is algorithmic stemming performing better than the standard analyzer?
2.   Why is algorithmic stemming performing better than hunspell stemming?
3.   Why is hunspell stemming performing worse that the standard analyzer on the Medline dataset?
4.   Why is hunspell stemming performing better that the algorithmic stemmer on the NPL dataset?

We will try to answer these questions by breaking down the results on the individual query and word matches.  
You can follow all steps by and reproduce our results by running our [comparison notebook](https://colab.research.google.com/github/pragmalingu/experiments/blob/master/01_Stemming/Evaluation/Comparison_Stemming_vs_Standard_Search.ipynb#scrollTo=X33-_O4rPMso).


### Algorithmic stemming improvements over the standard analyzer
We see from the gains figure that algorithmic stemming performs better than the standard analyzer on all testet datasets.
But how does the algorithmic stemmer achieve these results in practice?

We answer this by looking at the NPL dataset which shows a big gain of 19,65% for the stemmer token filter over the standard analyzer.

We first use our search analysis tool to figure out the queries that demonstrate [the biggest f1-score gains](https://colab.research.google.com/github/pragmalingu/experiments/blob/master/01_Stemming/Evaluation/Comparison_Stemming_vs_Standard_Search.ipynb#scrollTo=_fC_-pb7G4HM) with the stemmer token filter.
The three queries with the biggest fscore difference are 62, 73 and 52. We will be looking at query 62.


The query text of query 63 is `FAST TRANSISTOR COUNTERS`.

We first look at the disjoint true positives sets of the query results for both the algorithmic stemmer and standard analyzer. 
The highest ranked document for the algorithmic stemmer that is not present in the top k (20) result set of the standard analyzer is [document `8341`](https://colab.research.google.com/github/pragmalingu/experiments/blob/master/01_Stemming/Evaluation/Comparison_Stemming_vs_Standard_Search.ipynb#scrollTo=QjP7C2guRRhm).
For the standard analyzer the document [appears at position 34](https://colab.research.google.com/github/pragmalingu/experiments/blob/master/01_Stemming/Evaluation/Comparison_Stemming_vs_Standard_Search.ipynb#scrollTo=1JMpo-U2SWEW) which is outside of our k=20 result set that we use to calculate precision and recall.

The text of relevant document is: 
> nonsaturating pulse circuits using two junction transistors junction transistors are found to be fast enough for pulse applications if the collector voltage is 
> prevented from reaching zero switching times can be achieved with available types the required limiting action is effected by introducing diodes which terminate 
> the switching transients by their breakdown a two transistor binary counter is described

For the **algorithmic stemmer** the document is at position **3** and returns the following highlighted text:
> nonsaturating pulse circuits using two junction **transistors**  junction **transistors** are found to be **fast** (...)  a two **transistor** binary **counter** is described

The overall document score of **12.49** is the sum of these individual word scores:

|Word   | Score   | Term Frequency   | Document Frequency |
|-------|---------|------------------|--------------------|
|fast   | 4.31    | 1                | 84                 |
|transistor | 4.22| 3                | 640                |
|counter   | 3.95 | 1                | 127                |




For the **standard analyzer** the same document is at position **33** and returns the following highlighted text:
> nonsaturating pulse circuits using two junction transistors  junction transistors are found to be **fast** is effected by introducing diodes which terminate the switching transients by their breakdown  a two **transistor**

The overall document score of **7.10** is the sum of these individual word scores:

|Word   | Score   | Term Frequency   | Document Frequency |
|-------|---------|------------------|--------------------|
|fast   | 4.31    | 1                | 84                 |
|transistor | 2.78| 1                | 479                |

We can see that for the standard analyzer the inflected word **counters** is not matched with the word **counter** in the document. 
The occurences of the word **transistors** is also not matched with the stem **transistor** used in the query and therefore the term frequency and score of the word is lower.
The document frequency is also lower, which normally leads to higher score, but cannot make up for the lower term frequency in this instance.
<img alt="F1-Score Gains" src={useBaseUrl('img/EXP1_Query_62_explain.svg')} />


### Hunspell improvements over algorithmic stemming:

The NPL dataset is also the only dataset where our hunspell token filter performed slightly better than the algorithmic stemmer.
We can use our search analysis tool again to extract relevant examples from the datasets that showcase how the [hunspell stemmer achieves better results](https://colab.research.google.com/github/pragmalingu/experiments/blob/master/01_Stemming/Evaluation/Comparison_Stemming_vs_Standard_Search.ipynb#scrollTo=dkAJ64J3UnC_).

We start again by calculating the f-score differences for all queries between the hunspell stemmer and the algorithmic one.
We can then pick the query that shows the biggest f-score difference in favor of the hunspell stemmer, which is query `80`.

The query text of query 80 is `COULD YOU PLEASE GIVE ME ARTICLES ABOUT THE POSSIBILITIES OF GETTING RECTIFICATION USING METALLIC DEVICES`.

If we look at the disjoint set of true positives, we find that the hunspell stemmer finds document `8094` at position `1`, while the algorithmic stemmer positions this document at position `130`.

The text of document `8094` is:
>component design trends metallic rectifiers approach infinite life developments in cu se si ge and rectifiers are surveyed new designs give reduced size and longer life together with higher operating temperature output current and reverse voltage ratings

We can see that there are some strange tokens in the text `cu se si ge`, we double checked the original files of the dataset and insured that it is actually present there and not an artifact of our processing.

The Hunspell stemmer returns the following highlighted text:
> component design trends **metallic** **rectifiers** approach infinite life developments in cu se si ge and **rectifiers** are surveyed new designs **give** reduced size and longer life together with higher operating temperature'

The overall document score with the hunspell stemmr of **16.61** is the sum of these individual word scores:

|Word   | Score   | Term Frequency   | Document Frequency |
|-------|---------|------------------|--------------------|
|g or give   | 4.74    | 2           | 426                |
|rectification or  rectify| 5.98| 2  | 180                |
|metallic   | 5.88 | 1               | 47                 |

The algorithmic stemmer on the other hand returns the following highlighted text:
> component design trends **metallic** rectifiers approach infinite life developments in cu se si ge and rectifiers are surveyed new designs **give** reduced size and longer life together with higher operating temperature

The overall document score with the algorithmic stemmer of **7.27** is the sum of these individual word scores:

|Word   | Score   | Term Frequency   | Document Frequency |
|-------|---------|------------------|--------------------|
|give   | 3.45    | 1                | 426                |
|metal  | 3.82    | 1                | 300                |

These results show us that the algorimtic stemmer is missing the hit on the search term **RECTIFICATION** which appears in the document as **rectifiers**. 
We also see that the keyword **metallic** receives a better (lower) DF score with the hunspell stemmer because it is not stemmed to **metal**.

Why doesn't the algorithmic stemmer match **rectification** with **rectifiers**? We can use the analyze api endpoint of elasticsearch to see what the algorithmic stemmer makes of these words:
```
POST /pragmalingu-stemming-npl-corpus/_analyze
{
  "text": "rectification rectifiers rectify"
}
# Response:
{
  "tokens" : [
    {
      "token" : "rectif",
      "start_offset" : 0,
      "end_offset" : 13,
      "type" : "<ALPHANUM>",
      "position" : 0
    },
    {
      "token" : "rectifi",
      "start_offset" : 14,
      "end_offset" : 24,
      "type" : "<ALPHANUM>",
      "position" : 1
    },
    {
      "token" : "rectifi",
      "start_offset" : 25,
      "end_offset" : 32,
      "type" : "<ALPHANUM>",
      "position" : 2
    }
  ]
}
```
We can see that **rectification** gets overstemmed to **rectif** instead of **rectifi** and therefore is not matched with the other tokens.

But how exactly does the hunspell stemmer manage to match this keyword? 
We can also look at the output of the analyze api endpoint to answer this question:

Request:
```
POST /pragmalingu-hunspell-npl-corpus/_analyze
{
  "text": "rectification rectifiers rectify"
}
```
Response:
```
{
  "tokens" : [
    {
      "token" : "rectification",
      "start_offset" : 0,
      "end_offset" : 13,
      "type" : "<ALPHANUM>",
      "position" : 0
    },
    {
      "token" : "rectify",
      "start_offset" : 0,
      "end_offset" : 13,
      "type" : "<ALPHANUM>",
      "position" : 0
    },
    {
      "token" : "rectify",
      "start_offset" : 14,
      "end_offset" : 24,
      "type" : "<ALPHANUM>",
      "position" : 1
    },
    {
      "token" : "rectify",
      "start_offset" : 25,
      "end_offset" : 32,
      "type" : "<ALPHANUM>",
      "position" : 2
    }
  ]
}
```
How does the hunspell stemmer find the keyword "rectifiers"?

We can see that the search term **rectification** gets expanded to the synonym query: `Synonym(text:rectification text:rectify)`. 
**Rectify** seems to be the stem that the hunspell token filter found in [his dictionary](https://cgit.freedesktop.org/libreoffice/dictionaries/plain/en/en_US.dic) for the word **rectification** and also **rectifiers**. 
If we look at the [hunspell dictionary](https://cgit.freedesktop.org/libreoffice/dictionaries/plain/en/en_US.dic) we find the following entry: `rectify/XNDRSZG`, 
we can then look at the possible [suffix rules](https://cgit.freedesktop.org/libreoffice/dictionaries/plain/en/en_US.aff) that enable the hunspell stemmer to match these words.

The first relevant rule here is **N**:
``` 
SFX N Y 3  
SFX N e ion e  
SFX N y ication y  
SFX N 0 en [^ey]
```

The third line `SFX N y ication y` enables the matching of rectification.

But in the text **rectifiers** are mentioned and not **rectify** so we need an additional rule during indexing that matches **rectifiers** to the stem **rectify**.

The rule **Z** has a matching line for that:  
```
SFX Z Y 4  
SFX Z 0 rs e  
SFX Z y iers \[^aeiou\]y  
SFX Z 0 ers \[aeiou\]y  
SFX Z 0 ers \[^ey\]  
```

Here the thried line `SFX Z y iers [^aeiou]y` would match the word rectifiers and thus enable the token filter to replace it with the base entry in the dictionary.

There is a good explanation on how these hunspell dictionaries and affix files work on the [chromium developer documentation site](https://www.chromium.org/developers/how-tos/editing-the-spell-checking-dictionaries#TOC-Full-details).

### Algorithmic stemmer improvements over hunspell:

In most cases the algoritmic stemmer outperformes the hunspell stemmer.
We are looking at the CACM and ADI dataset which show the biggest differences to find examples that explain this performance.

First we look at the CACM dataset to find good examples.  
We calcuclate the f-score differences for both approaches with our search analysis tool and pick query **26** because it exhibits the biggest f-score difference between the two approaches.  

The query text of query 26 is:
>Concurrency control mechanisms in operating systems  

The first document in the disjoint set of true positives that is not found by the hunspell stemmer is document `1198`.

The text and title of the document are:  
*Title:*
>Solution of a Problem in Concurrent Programming Control  

*Text:*
>A number of mainly independent sequential-cyclic processes with restricted means of communication  with each other can be made in such a way that at any moment one and only one of them is engaged in the  "critical section" of its cycle. 

The document appears at position `3` for the algorithmic stemmer and at position `50` for the hunspell stemmer.

The algorithmic stemmer returns the following highlighted text:
> independent sequential-cyclic processes with restricted means of communication  with each other can be made **in** such a way that at any moment one and only one of them is engaged **in** the  "critical section" of its
And the following highlighted title:
>'Solution of a Problem **in** **Concurrent** Programming **Control**'

The overall document score with the algorithmic stemmer of **12.06** is the sum of the word scores in the **title** field:

|Word   | Field | Score   | Term Frequency   | Document Frequency |
|-------|-------|---------|------------------|--------------------|
|in     | text  | 0.40    | 2                | 1236               |
|concurr| title | 5.77    | 1                | 8                  |
|control| title | 4.30    | 1                | 38                 |
|in     | title | 1.98    | 1                | 416                |

Only the matches in the title field are counted for the overall document score.
This happens because we are using the default scoring strategie for the multi match query that is [**best_field**](https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-multi-match-query.html#multi-match-types), 
which takes the highest score of one of the searched fields as the documents overall score.

The hunspell stemmer on the other hand returns the following highlighted text:  
>independent sequential-cyclic processes with restricted means of communication with each other can be made **in** such a way that at any moment one and only one of them is engaged **in** the "critical section" of its  
And the following highlighted title:
>Solution of a Problem **in** Concurrent Programming **Control**  

The overall document score with the hunspell stemmer of **6.66** is also the sum of the word scores in the **title** field:

|Word   | Field | Score   | Term Frequency   | Document Frequency |
|-------|-------|---------|------------------|--------------------|
|in     | text  | 0.41    | 2                | 1236               |
|control| title | 4.62    | 1                | 31                 |
|in     | title | 2.04    | 1                | 416                |


The hunspell stemmer was not able to match the important keyword **Concurrency** with **Concurrent** in the document title.
We can look at the output of the analyze API endpoint of Elasticsearch to see what the hunspell stemmer makes of these two keywords:
Request:
```
POST /pragmalingu-hunspell-cacm-corpus/_analyze
{
  "text": "concurrency concurrent"
}
```
Response:
```
{
  "tokens" : [
    {
      "token" : "concurrency",
      "start_offset" : 0,
      "end_offset" : 11,
      "type" : "<ALPHANUM>",
      "position" : 0
    },
    {
      "token" : "current",
      "start_offset" : 12,
      "end_offset" : 22,
      "type" : "<ALPHANUM>",
      "position" : 1
    }
  ]
}
```
We can see that concurrency is not stemmed and concurrent seems to be overstemmed to current. We can look at the hunspell dictionary and affix file to see which rules lead to this outcome:  

For concurrency there is exactly one entry:  
`concurrency`  
For concurrent we find the following rule that seems so explain the overstemming to the term current  
`current/FAY`  
The rules **FAY** in the affix file are:
```
PFX A Y 1
PFX A   0     re         .

PFX F Y 1
PFX F   0     con        .

SFX Y Y 1
SFX Y   0     ly         .
```
This means that the words **current**, **recurrent**, **concurrent** and **currently** are all stemmed to **current**.

This seems to indicate that the hunspell dictionaries we are using that are freely available for spellchecking are not optimized for this stemmer use-case.

### Standard analyzer impovements over hunspell:

On the medline dataset the hunspell stemmer performed worse than the default standard analyzer. 
We are again looking at the biggest [f-score difference](https://colab.research.google.com/github/pragmalingu/experiments/blob/master/01_Stemming/Evaluation/Comparison_Stemming_vs_Standard_Search.ipynb#scrollTo=KLDIRhISQnls&line=2&uniqifier=1) 
in favor of the standard analyzer to find examples that explain this performance drawback of the hunspell stemmer.

Query 1 exhibits the biggest f-score difference between the standard analyzer and the hunspell stemmer.

The query text of query 1 is:
>the crystalline lens in vertebrates, including humans.  

The first document in the disjoint true positive set of query 1 is document `170`.
The text of document 170 is:
>identification of species-specific and organ-specific antigens in lens proteins. 
>the species-specific and organ-specific antigens of lens were investigated by gel diffusion and immunoelectrophoresis techniques. 
>it  was found that rabbit antiserum to bovine lens showed cross reaction with other bovine tissues.
>these cross-reacting antigens were the b- -  and y-crystallins .
>there were two major and a minor organ-specific antigen in lens .
>both the major antigens had a mobility and were        identified as the a-crystallin of lens .

The standard analyzer returns this document at position 9 with the following highlighted matched keywords:
> identification of species-specific and organ-specific antigens **in** **lens** proteins. 
>**the** species-specific and organ-specific antigens of **lens** were investigated by gel diffusion reaction with other bovine tissues. 
>these cross-reacting antigens were **the** b- -  and y-crystallins there were two major and a minor organ-specific
>antigen **in** **lens** . both **the** major antigens had a mobility and were identified as **the** a-crystallin of **lens**.

The overall document score with the standard analyzer was **6.22** and was the some of the following keyword scores: 

|Word   | Score   | Term Frequency   | Document Frequency |
|-------|---------|------------------|--------------------|
|the    | 0.022   | 4                | 2020               |
|lens   | 6.13    | 5                | 41                 |
|in     | 0.07    | 2                | 987                |

The hunspell stemmer instead positioned the document `170` at position 25 for query 1 and returned the following highlighted text:
>Identification of species-specific and organ-specific antigens **in** **lens** proteins. 
>**The** species-specific and organ-specific antigens of **lens** were investigated by gel diffusion and immunoelectrophoresis techniques.
>It was found that rabbit antiserum to bovine **lens** showed cross reaction with other bovine organ-specific 
>antigen **in** lens.Both **the** major antigens had a mobility and were identified as **the** a-crystallin of **lens**.

The overall document score with the hunspell stemmer was **5.378082** which is slightly lower than the standard analyzer score:

|Word   | Score   | Term Frequency   | Document Frequency |
|-------|---------|------------------|--------------------|
|the    | 0.026   | 4                | 1018               |
|l or lens | 5.27 | 8                | 75                 |
|in     | 0.07    | 2                | 986                |

We can see that nearly all of the score difference comes mostly from the different weighting of the hits for the term **lens**. 
The hunspell stemmer searches both with the original term lens as well as only with the letter l. 
This seems to come from the hunspell dictionary that contains the following entry:  
```
l/SDXTGJ  
```
Combined with the suffix rule X:  
```
SFX X Y 3
SFX X   e     ions       e
SFX X   y     ications   y
SFX X   0     ens        [^ey]
```
Which leads to the dictionary entry **l** as well as the entry `lens/MS` matching. 
Both terms combined have a higher document frequency of 75 than just the term **lens** where the document frequency is 41.

This is another indication that the freely available libre office dictionaries that we are using for the hunspell token filter are not 

## 4. Conclusion

For all our tested datasets algorithmic stemming showed considerable increases in f-score, ranging from 1.4-21.9%.
As algorithmic stemming is build in to the language analyzers provided by Elasticsearch it is fairly easy to use.
Due to it's rule based nature it doesn't come with a considerable processing or memory overhead.

Dictionary based stemming, as provided by the hunspell token filter, is not as easy to use, because you need a good dictionary that is tailored to the stemming tasks.
The analysis in our discusion section indicated that the freely available hunspell dictionaries that we used from the libreoffice project are not really suitable for stemming.
Hunspell dictionary stemming improved over the standard analyzer in all but one dataset, but it could not beat the algorithmic stemmer with the dictionaries we tested.

The overall conclusion and recommendations from our experiment is:
- Use the [language analyzers provided by Elasticsearch](https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-lang-analyzer.html) to get a quick and cheap boost in relevancy for your search use-case.
- Consider the [hunspell token filter](https://www.elastic.co/guide/en/elasticsearch/reference/current/analysis-hunspell-tokenfilter.html) only if you have access to a good dictionary and are sure that it will improve relevancy for your use-case.
- Don't choose the hunspell token filter with the freely available libreoffice dictionaries over the algoritmic stemmers.
