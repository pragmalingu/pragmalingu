---
id: ranking-api
title: How to Use the Elasticsearch Ranking Evaluation API
sidebar_label: Ranking Evaluation API
---

To evaluate our experiments we used the build-in [Ranking Evaluation API](https://www.elastic.co/guide/en/elasticsearch/reference/current/search-rank-eval.html) by Elasticsearch.
In order to adjust the Ranking Evaluation API to your needs, this guide will give you some short explainations of the operators and metrics we used during our experiments.

Before we start, the Ranking Evaluation API needs three ingredients to give us results back:

* Some documents you want to evaluate
* A few typical queries for those documents
* Relevance assessments that represent document ratings regarding the queries

At first we have to save the documents you want to search on to an Elasticsearch index. Detailed instructions on how to do that can be found in our [Elasticsearch Setup Guide](../guides/elastic-setup.mdx).

Next you need the search queries correctly formatted for the Ranking Evaluation API to read. We wrote down how you can achieve this from raw data in our [Parsing Guide](../guides/how-to-parse.mdx).

After parsing the queries, they should have the following format:

```json
{
    
  "requests": [
    {
      "id": "Query_1",                                  
      "request": {                                              
          "query": { "match": { "text": "\nwhat are the structural and aeroelastic problems associated with flight\nof high speed aircraft .\n" } }
      },
      "ratings": [                                              
        { "_index": "cranfield-corpus", "_id": "184", "rating": 1 },
        { "_index": "cranfield-corpus", "_id": "29", "rating": 1 },
        { "_index": "cranfield-corpus", "_id": "31", "rating": 1 }
      ]
    },
    {
      "id": "Query_2",
      "request": {
        "query": { "match": { "text": "\nwhat problems of heat conduction in composite slabs have been solved so\nfar .\n" } }
      },
      "ratings": [
        { "_index": "cranfield-corpus", "_id": "12", "rating": 1 }
      ]
    }
  ]
  "metric": {
    "precision": {
      "k": 20,
      "relevant_rating_threshold": 1,
      "ignore_unlabeled": false
    }
  }
}
```

On the [Python API Documentation website](https://elasticsearch-py.readthedocs.io/en/master/api.html) for the Ranking Evaluation API this is called the `body`so we will keep referring to it as the evaluation body.
After saving the body to a variable called `eval_body` we send it to the Ranking Evaluation API using json:

```python
eval_body = json.dumps(create_function)
# our elasticsearch instance is stored in the variable es
result = es.rank_eval(eval_body,index_we_want_to_search_on)
# print the evaluation in readable format
print(json.dumps(result, indent=4, sort_keys=True))
```

<details>
<summary>What is "create_function"?</summary>  

The argument `create_function` is a function we wrote to parse our preprocessed data into the evaluation body format. You can find details in our [Parsing Guide](https://pragmalingu.de/docs/guides/how-to-parse#parse-for-evaluation-body). 
</details> 

We specified on which index the Ranking Evaluation API should search. If you don't be aware that it will search on all indexes in your instance.

You can find all this information about the Ranking Evaluation API also on the [Elasticsearch website](https://www.elastic.co/guide/en/elasticsearch/reference/current/search-rank-eval.html).

## Query metrics

Depending on the data, it can help your search which query operator you choose. We will briefly explain the three operators that we tested by now.

### Match query operator

The [match query operator](https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-match-query.html) is the standard search operator in Elasticsearch. It's a full-text search in the given fields of the documents that comes with standard tokenization and an analyzer.

To use the match operator in the Ranking Evaluation API we need to add it to every query request in the field `"query":` and also add the query text to the field `"text":`. The operator tries to match the string given in `text:` on the documents in the given index.
Here is are some example queries:

```json
{
    
  "requests": [
    {
      "id": "Query_1",                                  
      "request": {                                              
          "query": { "match": { "text": "\nwhat are the structural and aeroelastic problems associated with flight\nof high speed aircraft .\n" } }
      },
      "ratings": [                                              
        ...
      ]
    },
    {
      "id": "Query_2",
      "request": {
        "query": { "match": { "text": "\nwhat problems of heat conduction in composite slabs have been solved so\nfar .\n" } }
      },
      "ratings": [
        ...
  ]
  "metric": {
    ...
  }
}
```


### Simple query string operator
 
The [simple query string](https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-simple-query-string-query.html) operator uses a parser with a limited error-tolerant syntax to search for the given query string in the given fields. It splits and analyzes the query terms separately for the search before returning the relevant documents. It ignores invalid parts of the query string. 

Here is an example request with the simple query string operator searching for the query in the 'text' and 'title' fields:

```json
{
  "requests": [
    {
      "id": "Query_1",                                  
      "request": {                                              
          "query": { "simple_query_string" : {
               "query": "\nwhat are the structural and aeroelastic problems associated with flight\nof high speed aircraft .\n",
               "fields": ["title", "text"]
      }}},
      "ratings": [                                              
        ...
      ]
    },
    {
      "id": "Query_2",
      "request": {
        "query": { "simple_query_string" : {
               "query": "\nwhat problems of heat conduction in composite slabs have been solved so\nfar .\n",
               "fields": ["title", "text"]
      }}},
      "ratings": [
        ...
  ]
  "metric": {
    ...
  }
}
```

### Multi-match query operator

The [multi-match query operator](https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-multi-match-query.html#query-dsl-multi-match-query) builds on the match query operator and allows multi-line match while searching. It does work similar to the simple string query but it rates the documents a little bit different.

If no specific fields are provided it will search on all fields available. We chose "title" and "text" since they have the most information:

```json
{
  "requests": [
    {
      "id": "Query_1",                                  
      "request": {                                              
          "query": { "multi_match" : {
               "query": "\nwhat are the structural and aeroelastic problems associated with flight\nof high speed aircraft .\n",
               "fields": ["title", "text"]
      }}},
      "ratings": [                                              
        ...
      ]
    },
    {
      "id": "Query_2",
      "request": {
        "query": { "multi_match" : {
               "query": "\nwhat problems of heat conduction in composite slabs have been solved so\nfar .\n",
               "fields": ["title", "text"]
      }}},
      "ratings": [
        ...
  ]
  "metric": {
    ...
  }
}
```

This operator was also the one we chose for our standard search to stemming comparison, which you can read in our [First Experiment](../experiments/experiment1.mdx). 

## Rating metric

With the rating metric we are able to provide better ratings for more relevant documents. For example if we search for `ice cream truck` we could rate documents with `ice cream truck` higher than documents with `ice cream` or `truck` and still label all those documents as relevant.
A document rating is represented by an integer. The higher the number the more relevant the document. To use this scale in your search we also have to adjust the `relevant_rating_threshold` variable to the highest rated number. The default would be 1, which means the different rated documents would all be irrelevant.

We can only make this adjustment if we have annotated data that provides the right metrics or if we are willing to rate the relevant documents manually. 

So if we don't have more than the information that a document is relevant or irrelevant, we can just use the binary scale. 0 is irrelevant and 1 is relevant. 

This is an example with higher scaled ratings:

```json
{
    
  "requests": [
    {
      "id": "Query_1",                                  
      "request": {                                              
          ...
      },
      "ratings": [                                              
        { "_index": "cranfield-corpus", "_id": "184", "rating": 2 },
        { "_index": "cranfield-corpus", "_id": "29", "rating": 3 },
        { "_index": "cranfield-corpus", "_id": "31", "rating": 1 }
      ]
    },
    {
      "id": "Query_2",
      "request": {
        ...
      },
      "ratings": [
        { "_index": "cranfield-corpus", "_id": "12", "rating": 1 }
      ]
    }
  ]
  "metric": {
    "precision": {
      "k": 20,
      "relevant_rating_threshold": 3,
      "ignore_unlabeled": false
    }
  }
}
```
Since the Cranfield corpus was the only one which provided scaled ratings, we only used the binary rating. We also didn't rate any documents with 0, since every document you don't rate gets the default value 0.

## Evaluation metrics

Aside from the metrics we can adjust by changing the request, there are some metrics that influence how the evaluation results will be computed.

### Precision at k
With setting the metric to `precision` and defining `k`, we can measure how many relevant results are returned in the top k results. 

Since the user only looks at the first few results of a search query, it doesn't make sense to set `k` higher than 20. Reducing the number of top results to 10 or 5 could give an even better idea of the performance.

But this metric only recognizes documents as relevant, if they are labeled. So it is possible that documents that would be relevant, but aren't labeled, would actually worsen the results on this.
Also the position isn't taken in account either. Documents on position 1 and position 10 are considered as equally important.

We already explained the `relevant_rating_threshold` parameter above. With the `ignore_unlabeled` parameter you are able to take the unlabeled documents into account. The default value is `false`, which treats unlabeled documents as irrelevant. But if set to `true`, those documents won't count at all.

An example for the Precision metric:

```json
{
    
  "requests": [
    ...
  ]
  "metric": {
    "precision": {
      "k": 20,
      "relevant_rating_threshold": 3,
      "ignore_unlabeled": false
    }
  }
}
```

<details>
<summary>What is "Precision"?</summary>  

Precision measures the probability that retrieved documents are relevant to the search query. Therefore the number of all retrieved relevant documents is divided by the number of all retrieved documents. <br />
For example: We retrieve 10 search results and only 5 are relevant for our search. 
The Precision measure would be: `5/10 = 0.5` <br />
To measure the Precision it's necessary to have the relevant documents labeled as such. 
Precision only looks at the documents that are retrieved and doesn't care about what more could've been retrieved.
</details> 

### Recall at k

The metric `recall` works very similar to the `precision` metric. `k` defines how many relevant results are returned in the top k results and taken into measurement. 
This metric also has difficulties, if documents aren't labeled.
The position isn't taken in account either. Which can be problematic since the Recall measure doesn't care about the retrieved irrelevant documents.

The `relevant_rating_threshold` parameter is explained above. 

An example for the Recall metric:

```json
{
    
  "requests": [
    ...
  ]
  "metric": {
    "recall": {
      "k": 20,
      "relevant_rating_threshold": 3
    }
  }
}
```

<details>
<summary>What is "Recall"?</summary>  

Recall measures the probability that relevant documents are retrieved. 
Therefore the number of all retrieved relevant documents is divided by the number all documents that are labeled as relevant. <br />
For example: We search on 10 documents, 8 of them are relevant and 4 of the relevant ones are retrieved. 
The Recall measure would be: `4/8 = 0.5` <br />
To measure the Recall it's necessary to have the relevant documents labeled.
Recall only looks at the documents that could be retrieved and doesn't take in account, if many irrelevant documents are retrieved aside the relevant ones.
</details> 

