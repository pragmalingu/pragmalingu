---
id: how-to-parse
title: How to Parse Data
sidebar_label: How to Parse for Elasticsearch
---

<div class="myDIV">Hover over me.</div>
<div class="hide">Here should be some deifinition.</div>

## Let's Start

In order to start experimenting and evaluating different Natural Language Processing (NLP) methods, it is necessary to bring the data you want to use into a suitable form.

Therefore you should start by answering the following three questions:
1. What ** does ** the data look like?
2. How do we ** want ** the data to look?
3. What kind of ** preproccesing ** would we want to aply first?

Since we are mainly working with Elasticsearch by trying to improve the search results, the third question is not relevant for us. Preprocessing such as tokenization, stemming, lemmatizing, ignoring stop words, etc. will always be applied to raw data. Since Elasticsearch works with automatic tokenization we won't cover tokenization at all. Preproccesing will be excluded for the time being. If you think it would be interested to expand our guide in that way, please don't hesitate [to write us about it](../about/team.md).

The code in the guide is prorgammed in Python3, since Python offers the best build-in methods for language processing out of all programming languages.

## Analyze Data 

**What does the Data look like?**

Before you can start parsing, you have to take a closer look at the data you want to parse.

It is relevant to determine which information is offered to you. Is it just text you're parsing? Is there additional information such as author, title, date or something similar? If there is additional information, is it labeled seperatly or is it just within the text?

The most difficult to parse are texts that do not provide any information about the parts of the data or that consist exclusively of non-stop text.

Since we decided to data sets that are specifically provided for NLP tasks we knew they would have a certain structure. So we only had to deal with the different characteristics of the data sets not with structuring it in the first place.

We downloaded all the data sets from the [University of Glasgow](http://ir.dcs.gla.ac.uk/resources/test_collections/), looked at them in detail and wrote detailed descriptions on the information each corpus provides.
So we were for example able to determine that the corpora [Cranfield](https://pragmalingu.de/docs/benchmarks/data-comparisson#cranfield), [CACM](https://pragmalingu.de/docs/benchmarks/data-comparisson#cacm), [CISI](https://pragmalingu.de/docs/benchmarks/data-comparisson#cisi) and [ADI](https://pragmalingu.de/docs/benchmarks/data-comparisson#adi) use very similar notations. The document text is always marked with a line that only contains ".W" at the beginning:

```
.W
one-dimensional transient heat conduction into a double-layer
slab subjected to a linear heat input for a small time
internal .
analytic solutions are presented for the transient heat
conduction in composite slabs exposed at one surface to a
triangular heat rate .  this type of heating rate may occur, for
example, during aerodynamic heating .
```

In contrast to the text, the additional information can thus be identified very clearly with the other markers provided.

If you don't want to determine the markers on your own, we have already done all the work by analyzing every single [Data Set](../benchmarks/data-comparisson.md) and even wrote an [overview table](../benchmarks/overview.md).

**How do we want the data to look?**

After we analyzed our data properly, we can move on to the parsing part. Depending on how you want to use the data, you have to adapt your code for the right format output.

In order to be able to work with Elasticsearch we first have to index all documents. You can read about what that exactly means and how to set up an Elasticsearch instance in our [Elasticsearch Guide](../guides/elastic-setup.md).

The documents we want to index need the following format:

```python
doc = {
    'author': 'kimchy',
    'text': 'Elasticsearch: cool. bonsai cool.',
    'title': 'Cool things',
}
```
Further information such as author, title, publication date, etc. can be added as searchable fields using the format ```'field_name': 'content'```. In order to improve the search results later on, it is definitely helpful to include as much additional information as possible.

In order to be able to index all documents in one go, we parsed our documents into a dictionary of dictionaries, which can easily be iterated later on.
The details will be covered in the parsing section.

For evaluation we use the [Elasticsearch Ranking API](https://elasticsearch-py.readthedocs.io/en/master/api.html?highlight=_rank_eval#elasticsearch.Elasticsearch.rank_eval). Therefore, we need some search queries and a list of documents, that are relevant for the queries, called relevance assessments. A description of how to use the Elasticsearch Ranking API can be found in our [Ranking API Guide](../guides/ranking-api.mdx).
The search queries, relevance assessments and the index name of the indexed documents are feed to the Ranking API as the evaluation body.

The evaluation body requires this form, in Python we simply used nested dictionaries:

```python
eval_body = {
    "requests": [
        {
            "id": "\nwhat are the structural and aeroelastic problems associated with flight\nof high speed aircraft .\n",
            "request": {
                "query": { "match": { "text": "berlin" }}
            },
            "ratings": [
                { "_index": "pragmalingu-cranfield-corpus", "_id": "184", "rating": 1 },
                { "_index": "pragmalingu-cranfield-corpus", "_id": "29", "rating": 1 },
                [...]
            ]
        },
        {
            "id": "Query_2",
            "request": {
                "query": { "match": { "text":  "text": "\nwhat problems of heat conduction in composite slabs have been solved so\nfar .\n"}}
            },
            "ratings": [
                { "_index": "pragmalingu-cranfield-corpus", "_id": "12", "rating": 1 },
                { "_index": "pragmalingu-cranfield-corpus", "_id": "15", "rating": 1 },
                [...]
            ]
        },
        [...]
    ],
    "metric": {
      "precision": {
        "k" : 20,
        "relevant_rating_threshold": 1,
        "ignore_unlabeled": "false"
      }
   }
```

## Parse Data

Now that we've figured out what the data looks like and how we want it to be in the end, we can finally get started with parsing.
Since it's easier to show parsing by using a practical example, we picked one of our [8 free avaiable Data Sets](../benchmarks/overview.md). 
We chose the [Cranfield Corpus](https://pragmalingu.de/docs/benchmarks/data-comparisson#cranfield). On the one hand, because it uses the same notation as CACM, CISI and ADI, therefore the code can easily be adapted to parse the other corpora as well. 
(That's the reason we also started parsing the Cranfield corpus for our experiments) 
On the other hand, it contains a lot of additional information and at the same time didn't cause major problems while parsing. 
So it seems to be a perfect example to show you our approach step by step. 
In the last section, we will also talk about some of the problems we had while parsing the other corpora.
So let's just dive into it!

#### Cranfield Corpus

You can get the corpus from [this link](http://ir.dcs.gla.ac.uk/resources/test_collections/cran/).
For detailed information about the format of the files, see the PragmaLingu [ Benchmarks](https://pragmalingu.de/docs/benchmarks/overview).


#### Process the documents

First of all we need to download the data and make it accessible in out code. 
The entries in the Cranfield corpus are only seperated by the lines that start with '\\.' followed by a letter to tell us which part of the entry it is. 
(Definition for the markers can be found [here](https://pragmalingu.de/docs/benchmarks/data-comparisson#cranfield)) The only exception is the ID-tag where the ID is in the same row as the tag. We can use this to seperate the text into it's entries as a first step. For easier use we just going to split the text which transforms it into a list of entries. We can do the same for the queries since they follow the same notation:

```python
ID_marker = re.compile('\.I.')

def get_data(PATH_TO_FILE, marker):
  """
  Reads file and spilts text into entries at the ID marker '.I'.
  First entry is empty, so it's removed.
  'marker' contains the regex at which we want to split
  """
  with open (PATH_TO_FILE,'r') as f:
    text = f.read().replace('\n'," ")
    lines = re.split(marker,text)
    lines.pop(0)
  return lines

txt_list = get_data(PATH_TO_CRAN_TXT, ID_marker)
qry_list = get_data(PATH_TO_CRAN_QRY, ID_marker)
```

Afterwards we can try to split the information of the entries in seperate parts of a dictionary. To make the indexing into elsatic search easier we are going to store all documents in one parsebale dictionary of dictionaires:

```python
from collections import defaultdict
import re

chunk_start = re.compile('\.[A,B,T,W]')
txt_data = defaultdict(dict)

for line in txt_list:
  entries= re.split(chunk_start,line)
  id = entries[0].strip()
  title = entries[1]
  author = entries[2]
  publication_date = entries[3]
  text = entries[4]
  txt_data[id]['title'] = title
  txt_data[id]['author'] = author
  txt_data[id]['publication_date'] = publication_date
  txt_data[id]['text'] = text
```

This corpus is very good notated but we have to be careful, sometimes there are corpora where not every entry has the same information. For example if some entries don't have an athour or no publication date, we have to change the code likewise and try to prepare for this different cases. Most of the times it's not this easy.

After we parsed all our documents we can now parse them to the elasticsearch index one by one:

```python
cran_index = "cranfield-corpus"

for ID, doc_data in txt_data.items():
  es.index(index=cran_index, id=ID, body=doc_data)
```

#### Process the queries and relevance assessments

With the queries it's similar to the documents. We start parsing their entries with the ID-tag and continue with splitting every entry into smaller pieces. The queries only consist of the ID and the query, so it's way easier to get that information:

```python
from collections import defaultdict
import re

chunk_start = re.compile('\.[W]')
qry_data = defaultdict(dict)

for n in range(0,len(qry_list)-1):
  line = qry_list[n+1]
  _ , question = re.split(chunk_start,line)
  qry_data[n+1]['question'] = question
```

If we don't need the rating that is provided by the Cranfield corpus we can simply parse the file by splitting it into lines and only save the query-ID and the document-ID:

```python
cran_rel = defaultdict(list)

with open (PATH_TO_CRAN_REL,'r') as f:
  for line in f:
    line = re.split(' ',line)
    cran_rel[int(line[0])].append(line[1])
```

Otherwise it would also be possible to parse the file as a numpy array for easier accessability:

```python
cran_rel_data = open(PATH_TO_CRAN_REL)
cran_np = np.loadtxt(cran_rel_data, dtype=int)

cran_rel_rat = defaultdict(list)
for row in cran_np:
  cran_rel_rat[row[0]].append(tuple(row[1:])) 
```

Afterwards we can pass the data into a nested dictionary using this function:

```python
cran_index = "cranfield-corpus"

def create_query_body(query_dict, rel_dict, index_name):
  """
  The function creates a request for every query in query_dict and rates the relevant documents with rel_dict to 1.
  The index name has to be the same as from the documents your looking at.
  An evaluation body for the elasticsearch ranking API is returned.
  """
  eval_body = {
      "requests":'',
      "metric": {
          "precision": {
              "k" : 20,
              "relevant_rating_threshold": 1,
              "ignore_unlabeled": "false"
              }
      }
  }
  requests = [] 
  current_request = defaultdict(lambda: defaultdict())
  current_rel = {"_index": index_name, "_id": '', "rating": int}
  for query_ID, query_txt in query_dict.items():
    current_query = {"query": { "match": { "text": '' }}}
    current_query["query"]["match"]["text"] = query_txt['question']
    current_request["id"] = 'Query_'+str(query_ID)
    current_request["request"] = current_query.copy()
    current_request["ratings"] = [{"_index": index_name, "_id": str(el), "rating": 1} for el in rel_dict[query_ID]]
    requests.append(current_request.copy())
  eval_body["requests"] = requests
  return eval_body
```

## Problems
