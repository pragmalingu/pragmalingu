---
id: how-to-parse
title: How to Parse Data
sidebar_label: How to Parse for Elasticsearch
---

## Let's Start

In order to start experimenting and evaluating different Natural Language Processing (NLP) methods, it is necessary to bring the data you want to use into a suitable form.

Therefore you should start by answering the following three questions:
1. What ** does ** the data look like?
2. How do we ** want ** the data to look?
3. What kind of ** preproccesing ** would we want to aply first?

Since we are mainly working with Elasticsearch by trying to improve the search results, the third question is not relevant for us. Preprocessing such as tokenization, stemming, lemmatizing, ignoring stop words, etc. will always be applied to raw data. Since Elasticsearch works with automatic tokenization we won't cover tokenization at all. Preproccesing will be excluded for the time being. If you think it would be interested to expand our guide in that way, please don't hesitate [to write us about it](../about/team.md).

The code in the guide is prorgammed in Python3, since Python offers the best build-in methods for language processing out of all programming languages.

## Analyze Data 

**What does the Data look like?**
Before you can start parsing, you have to take a closer look at the data you want to parse.

It is relevant to determine which information is offered to you. Is it just text you're parsing? Is there additional information such as author, title, date or something similar? If there is additional information, is it labeled seperatly or is it just within the text?

The most difficult to parse are texts that do not provide any information about the parts of the data or that consist exclusively of non-stop text.

Since we decided to data sets that are specifically provided for NLP tasks we knew they would have a certain structure. So we only had to deal with the different characteristics of the data sets not with structuring it in the first place.

We downloaded all the data sets from the [University of Glasgow](http://ir.dcs.gla.ac.uk/resources/test_collections/), looked at them in detail and wrote detailed descriptions on the information each corpus provides.
So we were for example able to determine that the corpora [Cranfield](https://pragmalingu.de/docs/benchmarks/data-comparisson#cranfield), [CACM](https://pragmalingu.de/docs/benchmarks/data-comparisson#cacm), [CISI](https://pragmalingu.de/docs/benchmarks/data-comparisson#cisi) and [ADI](https://pragmalingu.de/docs/benchmarks/data-comparisson#adi) use very similar notations. The document text is always marked with a line that only contains ".W" at the beginning:

```
.W
one-dimensional transient heat conduction into a double-layer
slab subjected to a linear heat input for a small time
internal .
analytic solutions are presented for the transient heat
conduction in composite slabs exposed at one surface to a
triangular heat rate .  this type of heating rate may occur, for
example, during aerodynamic heating .
```

In contrast to the text, the additional information can thus be identified very clearly with the other markers provided.

If you don't want to determine the markers on your own, we have already done all the work by analyzing every single [Data Set](../benchmarks/data-comparisson.md) and even wrote an [overview table](../benchmarks/overview.md).

**How do we want the data to look?**
After we analyzed our data properly, we can move on to the parsing part. Depending on how you want to use the data, you have to adapt your code for the right format output.

In order to be able to work with Elasticsearch we first have to index all documents. You can read about what that exactly means and how to set up an Elasticsearch instance in our [Elasticsearch Guide](../guides/elastic-setup.md).

The documents we want to index need the following format:

```python
doc = {
    'author': 'kimchy',
    'text': 'Elasticsearch: cool. bonsai cool.',
    'title': 'Cool things',
}
```

Further information such as author, title, publication date, etc. can be added as searchable fields using the format ```'field_name': 'content'```. In order to improve the search results later on, it is definitely helpful to include as much additional information as possible.

In order to be able to index all documents in one go, we parsed our documents into a dictionary of dictionaries, which can easily be iterated later on.
The details will be covered in the parsing section.

For evaluation we use the [Elasticsearch Ranking API](https://elasticsearch-py.readthedocs.io/en/master/api.html?highlight=_rank_eval#elasticsearch.Elasticsearch.rank_eval). Therefore, we need some search queries and a list of documents, that are relevant for the queries, called relevance assessments. A description of how to use the Elasticsearch Ranking API can be found in our [Ranking API Guide](../guides/ranking-api.mdx).
The search queries, relevance assessments and the index name of the indexed documents are feed to the Ranking API as the evaluation body.

The evaluation body requires this form, in Python we simply used nested dictionaries:

```python
eval_body = {
    "requests": [
        {
            "id": "Query_1",
            "request": {
                "query": { "match": { "text": "\nwhat are the structural and aeroelastic problems associated with flight\nof high speed aircraft .\n" }}
            },
            "ratings": [
                { "_index": "pragmalingu-cranfield-corpus", "_id": "184", "rating": 1 },
                { "_index": "pragmalingu-cranfield-corpus", "_id": "29", "rating": 1 },
                [...]
            ]
        },
        {
            "id": "Query_2",
            "request": {
                "query": { "match": { "text":  "text": "\nwhat problems of heat conduction in composite slabs have been solved so\nfar .\n"}}
            },
            "ratings": [
                { "_index": "pragmalingu-cranfield-corpus", "_id": "12", "rating": 1 },
                { "_index": "pragmalingu-cranfield-corpus", "_id": "15", "rating": 1 },
                [...]
            ]
        },
        [...]
    ],
    "metric": {
      "precision": {
        "k" : 20,
        "relevant_rating_threshold": 1,
        "ignore_unlabeled": "false"
      }
   }
```

## Parse Data

Now that we've figured out what the data looks like and how we want it to be in the end, we can finally get started with parsing.
Since it's easier to show parsing by using a practical example, we picked one of our [8 free avaiable Data Sets](../benchmarks/overview.md). 
We chose the [Cranfield Corpus](https://pragmalingu.de/docs/benchmarks/data-comparisson#cranfield). On the one hand, because it uses the same notation as CACM, CISI and ADI, therefore the code can easily be adapted to parse the other corpora as well. 
(That's the reason we also started parsing the Cranfield corpus for our experiments) 
On the other hand, it contains a lot of additional information and at the same time didn't cause major problems while parsing. 
So it seems to be a perfect example to show you our approach step by step. 
In the last section, we will also talk about some of the problems we had while parsing the other corpora.
So let's just dive into it!

#### Cranfield Corpus

You can get the corpus from [this link](http://ir.dcs.gla.ac.uk/resources/test_collections/cran/).
For detailed information about the format of the files, see the PragmaLingu [ Benchmarks](https://pragmalingu.de/docs/benchmarks/overview).
In Colab you have to add the following lines:

```python
!wget http://ir.dcs.gla.ac.uk/resources/test_collections/cran/cran.tar.gz
!tar -xf cran.tar.gz
```

After downloading it you should make sure, that you set the paths for every file you want to parse:
```python
PATH_TO_CRAN_TXT = '/content/cran.all.1400'
PATH_TO_CRAN_QRY = '/content/cran.qry'
PATH_TO_CRAN_REL = '/content/cranqrel'
```

### Parse for Indexing

To create an index in Elasticsearch for the Cranfield corpus we only need the documents file first. In our case this is the file `cran.all.1400`.
Conveniently, every new document begins with the ID marker '.I', directly followed by the ID. Therefore, we can start by splitting all entries at this marker.
(Detailed definitions of all the markers can be found [here] (https://pragmalingu.de/docs/benchmarks/data-comparisson#cranfield))

We get a list of all the document entries:

```python
ID_marker = re.compile('\.I.')

def get_data(PATH_TO_FILE, marker):
  """
  Reads file and spilts text into entries at the ID marker '.I'.
  First entry is empty, so it's removed.
  'marker' contains the regex at which we want to split
  """
  with open (PATH_TO_FILE,'r') as f:
    text = f.read().replace('\n'," ")
    lines = re.split(marker,text)
    lines.pop(0)
  return lines

txt_list = get_data(PATH_TO_CRAN_TXT, ID_marker)
qry_list = get_data(PATH_TO_CRAN_QRY, ID_marker)
```

As the search queries are also separated by the ID marker, we can split them up in the same step. But we only need the qry_list later.

Afterwards we can split the information of every entry using the other information markers. To make the indexing into elsatic search easier we are going to store all documents in one parsebale dictionary of dictionaires as follows:

```python
from collections import defaultdict
import re

chunk_start = re.compile('\.[A,B,T,W]')
txt_data = defaultdict(dict)

for line in txt_list:
  entries= re.split(chunk_start,line)
  id = entries[0].strip()
  title = entries[1]
  author = entries[2]
  publication_date = entries[3]
  text = entries[4]
  txt_data[id]['title'] = title
  txt_data[id]['author'] = author
  txt_data[id]['publication_date'] = publication_date
  txt_data[id]['text'] = text
```

This corpus is very good notated but we have to be careful, sometimes there are corpora where not every entry has the same information. For example if some entries don't have an athour or no publication date, we have to change the code likewise and try to prepare for this different cases. But we're going to look into that later.

After we parsed all our documents we can now parse them to the elasticsearch index one by one:

```python
cran_index = "cranfield-corpus"

for ID, doc_data in txt_data.items():
  es.index(index=cran_index, id=ID, body=doc_data)
```

Now that the index is set we can start parsing the evaluation body for the Ranking API.

### Parse for Evaluation Body

**Queries**
Since we already split the queries into a list in the same step as the documents, we can continue with using the qry_list.

With the queries it's similar to the documents. We start parsing their entries with the ID-tag and continue with splitting every entry into smaller pieces. The queries only consist of the ID and the query, so it's way easier to get that information:

```python
from collections import defaultdict
import re

chunk_start = re.compile('\.[W]')
qry_data = defaultdict(dict)

for n in range(0,len(qry_list)-1):
  line = qry_list[n+1]
  _ , question = re.split(chunk_start,line)
  qry_data[n+1]['question'] = question
```

Counting the queries with the range operator was an easy way to do this, but of course it would also work to split the ID and transform it to an int.

**Relevance Assessments**
If we don't need the special rating that is provided by the Cranfield corpus (read about that [here](https://pragmalingu.de/docs/benchmarks/data-comparisson#cranfield)) we can simply parse the file by splitting it into lines and only save the query-ID and the document-ID:

```python
cran_rel = defaultdict(list)

with open (PATH_TO_CRAN_REL,'r') as f:
  for line in f:
    line = re.split(' ',line)
    cran_rel[int(line[0])].append(line[1])
```

Otherwise it would also be possible to parse the file as a numpy array for easier accessability on the additional ratings:

```python
cran_rel_data = open(PATH_TO_CRAN_REL)
cran_np = np.loadtxt(cran_rel_data, dtype=int)

cran_rel_rat = defaultdict(list)
for row in cran_np:
  cran_rel_rat[row[0]].append(tuple(row[1:])) 
```

***Evaluation Body***

Now that we have all the data ready, we can feed it to this function to create an evaluation body, with whom we can start evaluating on the Ranking API:

```python
cran_index = "cranfield-corpus"

def create_query_body(query_dict, rel_dict, index_name):
  """
  The function creates a request for every query in query_dict and rates the relevant documents with rel_dict to 1.
  The index name has to be the same as from the documents your looking at.
  An evaluation body for the elasticsearch ranking API is returned.
  """
  eval_body = {
      "requests":'',
      "metric": {
          "precision": {
              "k" : 20,
              "relevant_rating_threshold": 1,
              "ignore_unlabeled": "false"
              }
      }
  }
  requests = [] 
  current_request = defaultdict(lambda: defaultdict())
  current_rel = {"_index": index_name, "_id": '', "rating": int}
  for query_ID, query_txt in query_dict.items():
    current_query = {"query": { "match": { "text": '' }}}
    current_query["query"]["match"]["text"] = query_txt['question']
    current_request["id"] = 'Query_'+str(query_ID)
    current_request["request"] = current_query.copy()
    current_request["ratings"] = [{"_index": index_name, "_id": str(el), "rating": 1} for el in rel_dict[query_ID]]
    requests.append(current_request.copy())
  eval_body["requests"] = requests
  return eval_body

# create the eval_body
cran_create = create_query_body(cran_qry_data, cran_rel, cran_index)
# dump the body to json and feed it to Ranking API
cran_eval_body = json.dumps(cran_create)
cran_res = es.rank_eval(cran_eval_body, cran_index)
# print the results
print(json.dumps(cran_res, indent=4, sort_keys=True))
```

And that was about it. As already mentioned, it is not always that easy with every corpus. Therefore, in the next section, we would like to introduce you to a few problems that we have encountered and that can save you a lot of time if you consider them in advance.

## Problems

Hereafter we will briefly present some of the most noticeable problems from the 8 parsed corpora. Those can become problems while parsing other corpora. It will show you how important it is to analyse the data beforehand.

### Splitted Files

Sometimes not all the data is collected in one file. For us the only corpus in which this was the case, was LISA and it was solved very quickly. But splitted documents can lead to additional work and therefore should be viewed beforehand. Otherwise it can happen that not all data are taken into account while parsing.
The LISA corpus is split into 14 different files which contain the 6004 document entries.

We solved it with parsing the file names with a regex:

```python
file_regex = re.compile('LISA[0-5]')
lisa_files = [i for i in os.listdir(PATH_TO_LISA_TXT) if os.path.isfile(os.path.join(PATH_TO_LISA_TXT,i)) and \
         re.match(file_regex,i)]

for name in lisa_files: 
  lisa_txt_list.extend(get_data(PATH_TO_LISA_TXT+name, txt_entry_marker))
```

### Information Inconsistencies

Within parsing the ADI, CACM and CISI another common problem showed. Although they have notations very similar to Cranfield, unfortunately some inconsistencies caused problems during parsing. Since the data is annotated manually it can happen that for some entries the annotater just skiped the information tag if there wasn't any information to write. This caused code errors especially while parsing the document files.
After initially assuming that we have a certain amount of information for each entry, we quickly discovered that sometimes there was for example no author tag and other times more than one.
We will now take a quick look at some example problems and how we soved them:

With 83 entries, ADI is the smallest of the corpora and therefore very manageable.
In the ADI, each entry should have information on ID (.I), title (.T), text (.W) and author (.A). Unfortunately, the information about the author and the tag (.A) are missing in some entries:

```
.I 7
.T
a computer oriented photo-composition system for star
.W
the system for producing nasa's scientific
and technical aerospace reports, an example of computer
oriented automatic photo copy setting, produces copy
 at three times the speed of manual operation .  the discussion
is limited to a punched paper tape application and
 does not cover aspects of a more sophisticated computer
 system.
```

Our code quickly threw an error, so to get around this problem, after saving the ID, we added an if query:

```python
adi_title_start = re.compile('\.T')
adi_author_start = re.compile('\.A')
adi_text_start = re.compile('\.W')

# process the document data

adi_txt_data = defaultdict(dict)

for line in adi_txt_list:
  entries = re.split(adi_title_start,line,1)
  id = entries[0].strip()
  no_id = entries[1]
  if len(re.split(adi_author_start, no_id,1)) > 1:
    no_id_entries = re.split(adi_author_start, no_id,1)
    adi_txt_data[id]['title'] = no_id_entries[0]
    no_title = no_id_entries[1]
    no_title_entries = re.split(adi_text_start, no_title)
    adi_txt_data[id]['author'] = no_title_entries[0]
    adi_txt_data[id]['text'] = no_title_entries[1]
  else:
    no_id_entries = re.split(adi_text_start, no_id)
    adi_txt_data[id]['title'] = no_id_entries[0]
    adi_txt_data[id]['text'] = no_id_entries[1]
```

A similar problem can be found in the CACM. Each entry should hold the information on ID (.I), title (.T), text (.W), publication date (.B), author (.A), adding date (.N) and cross-references (.X). However, not every entry was assigned a text and an author.

```
.I 4
.T
Glossary of Computer Engineering and Programming Terminology
.B
CACM November, 1958
.N
CA581103 JB March 22, 1978  8:32 PM
.X
4	5	4
4	5	4
4	5	4
```

Since the entries without text and the entries without the author are not always the same, we had to include two nested if queries for parsing this corpus:

```python
# process text file

cacm_chunk_title = re.compile('\.[T]\n')
cacm_chunk_txt = re.compile('\n\.W\n') # not enough
cacm_chunk_txt_pub = re.compile('\.[W,B]')
cacm_chunk_publication = re.compile('\.[B]\n')
cacm_chunk_author = re.compile('^\.[A]\n', re.MULTILINE)
cacm_chunk_author_add_cross = re.compile('^\.[A,N,X]\n',re.MULTILINE) # not enough
cacm_chunk_add_cross = re.compile('\.[B,N,X]\n')

# process the document data

cacm_txt_data = defaultdict(dict)

for line in cacm_txt_list:
  entries= re.split(cacm_chunk_title,line)
  id = entries[0].strip() #save id
  no_id = entries[1]

  if len(re.split(cacm_chunk_txt, no_id)) == 2: # is there text
    no_id_entries = re.split(cacm_chunk_txt_pub, no_id,1)
    cacm_txt_data[id]['title'] = no_id_entries[0].strip() # save title
    cacm_txt_data[id]['text'] = no_id_entries[1].strip() # save text
    no_title_txt = no_id_entries[1]

    if len(re.split(cacm_chunk_author, no_title_txt)) == 2: # is there a auhtor
      no_title_entries = re.split(cacm_chunk_author_add_cross, no_title_txt)
      cacm_txt_data[id]['publication_date'] = no_title_entries[0].strip() # save publication date
      cacm_txt_data[id]['author'] = no_title_entries[1].strip() # save athor
      cacm_txt_data[id]['add_date'] = no_title_entries[2].strip() # save add date
      cacm_txt_data[id]['cross-references'] = no_title_entries[3].strip() # save cross-references

    else:
      no_title_entries = re.split(cacm_chunk_publication, no_title_txt)
      cacm_txt_data[id]['publication_date'] = no_title_entries[0].strip() # save publication date
      cacm_txt_data[id]['add_date'] = no_title_entries[1].strip() # save add date
      cacm_txt_data[id]['cross-references'] = no_title_entries[1].strip() # save cross-references

  else:
    no_id_entries = re.split(cacm_chunk_publication, no_id,1)
    cacm_txt_data[id]['title'] = no_id_entries[0].strip() # save title
    no_title = no_id_entries[1]

    if len(re.split(cacm_chunk_author, no_title,1)) == 2: # is there a auhtor
      no_title_entries = re.split(cacm_chunk_author_add_cross, no_title)
      cacm_txt_data[id]['publication_date'] = no_title_entries[0].strip() # save publication date
      cacm_txt_data[id]['author'] = no_title_entries[1].strip() # save athor
      cacm_txt_data[id]['add_date'] = no_title_entries[2].strip() # save add date
      cacm_txt_data[id]['cross-references'] = no_title_entries[3].strip() # save cross-references

    else:
      no_title_entries = re.split(cacm_chunk_add_cross, no_title)
      cacm_txt_data[id]['publication_date'] = no_title_entries[0].strip() # save publication date
      cacm_txt_data[id]['add_date'] = no_title_entries[1].strip() # save add date
      cacm_txt_data[id]['cross-references'] = no_title_entries[2].strip() # save cross-references
```

The CISI also had the problem that not all information was always given. Every entry should contain ID (.I), title (.T), text (.W), publication date (.B), author (.A) and cross-references (.X). But some of the entries had more than one author tag. These entries overlapped with the entries for which the information on the publication date was missing:

```
.I 33
.T
The "Half-Life" of Some Scientific and Technical Literatures
.A
Burton, R.E.
.A
Kebler, R.W.
.W
   A consideration of the analogy between the half-life of radioactive 
substances and the rate of obsolescence of scientific literature.. The validity 
of this analogy suggest the possibility of more accurate prognostications
concerning the period of time during which scientific literature may by used 
and hence might help to guide the planning of library collections and 
technical information services..
.X
33	19	33
36	2	33
41	1	33
```

Therefore we only needed one nested if query:

```python
cisi_title_start = re.compile('[\n]\.T')
cisi_author_start = re.compile('[\n]\.A')
cisi_date_start = re.compile('[\n]\.B')
cisi_text_start = re.compile('[\n]\.W')
cisi_cross_start = re.compile('[\n]\.X')

# process the document data

cisi_txt_data = defaultdict(dict)

for line in cisi_txt_list:
  entries = re.split(cisi_title_start,line,1)
  id = entries[0].strip()#save the id
  no_id = entries[1] 
  
  if len(re.split(cisi_author_start, no_id)) >= 2: # is there just one author?
    no_id_entries = re.split(cisi_author_start, no_id,1)
    cisi_txt_data[id]['title'] = no_id_entries[0].strip() # save title
    no_title = no_id_entries[1]

    if len(re.split(cisi_date_start, no_title)) > 1: # is there a publication date?
      no_title_entries = re.split(cisi_date_start, no_title)
      cisi_txt_data[id]['author'] = no_title_entries[0].strip() # save athour
      no_author = no_title_entries[1]
      no_author_entries = re.split(cisi_text_start, no_author)
      cisi_txt_data[id]['publication_date'] = no_author_entries[0].strip() # save publication date
      no_author_date = no_author_entries[1]
    else:
      no_title_entries = re.split(cisi_text_start, no_title)
      cisi_txt_data[id]['author'] = no_title_entries[0].strip() # save athour
      no_author_date = no_title_entries[1]

  else:
    no_id_entries = re.split(cisi_author_start, no_id)
    cisi_txt_data[id]['title'] = no_id_entries[0].strip() # save title
    cisi_txt_data[id]['author'] = no_id_entries[1].strip() # save first author
    no_title_entries = re.split(cisi_text_start, no_title)
    cisi_txt_data[id]['author'] += ','+no_title_entries[0].strip() # save second athour
    no_author_date = no_title_entries[1]

  last_entries = re.split(cisi_cross_start, no_author_date)
  cisi_txt_data[id]['text'] = last_entries[0].strip() # save text
  cisi_txt_data[id]['cross-refrences'] = last_entries[1].strip() # save cross refrences
```

These examples show you how important it is to know your data when parsing and to be prepared for any irregularities, especially when it comes to manually annotated data.

### Unexpected Text Seperators

In addition to the missing information, an irregularity in the distance markers can also lead to a lot of frustration while parsing. The document and the query files of LISA and NPL where a bit inconsitent with spaces around the entry markers.

For the LISA corpus irregular spaces have become a problem both in the document files and in the query files.
As with most corpora, you could split the entries using a marker and then save the information in a dictionary.
With LISA however the entries looked like this:

```
Document  501
LIBRARY AND INFORMATION SERVICES FOR THE PUBLIC: PROCEEDINGS OF THE 8TH
CONFERENCE OF THE PAPUA NEW GUINEA LIBRARY ASSOCIATION.

THE CONFERENCE WAS HELD AT THE ADMINISTRATIVE COLLEGE OF PAPUA NEW GUINEA,
WAIGANI, PORT MORESBY, 18-19 OCT 79. PAPERS ARE IN 5 SECTIONS: STRATEGIES IN
THE PLANNING OF LIBRARY AND INFORMATION SERVICES; STATE OF LIBRARIES FOR THE
PUBLIC IN PAPUA NEW GUINEA; LITERACY AND THE LIBRARY; REACHING OUT: ACCESS TO
LIBRARY-BASED INFORMATION SERVICES IN THE RURAL AREAS OF PAPUA NEW GUINEA; AND
AGENCIES INVOLVED IN INFORMATION WORK. TRANSCRIPTIONS OF QUESTION AND ANSWER
SESSIONS ARE ALSO INCLUDED.
********************************************
```

The entries were separated in by splitting at the `txt_entry_marker` and all empty lines were removed from the resulting list. Every document contains information on the ID, title and text. To get the ID we removed the string `Document` beforehand. For the title, the 1-2 newlines with the regex `doc_strip` were replaced by an empty string, as this made the empty line between the title and text an empty entry. Using the indexes of these empty entries, we were then able to save the title and text separately:

```python
txt_entry_marker = re.compile('\*{44}',re.MULTILINE)

doc_strip = re.compile('\n?Document {1,2}')

lisa_txt_list_stripped = []
lisa_txt_data = defaultdict(dict)

for el in lisa_txt_list:
  lisa_txt_list_stripped.append(re.sub(doc_strip,'', el))

for entry in lisa_txt_list_stripped:
  parts = entry.split('\n')
  empty_index = parts.index('')
  ID = parts[0]
  title = parts[1:empty_index]
  text = parts[empty_index+1:]
  lisa_txt_data[ID]['title'] = title
  lisa_txt_data[ID]['text'] = text
```

The query files were a little easier to handle, when splitting on the newlines, it was only necessary to handle the first line separately because it does not start with a newline:

```python
# process the query data
lisa_qry_data = defaultdict(dict)

# first line is a special case because it doesn't start with a newline
first_line = lisa_qry_list[0]
first_question = ' '.join(first_line[1:])
lisa_qry_data[int(first_line[0])]['question'] = first_question

# after that every line can be handle the same way
for n in range(0,len(lisa_qry_list)-1):
  line = re.split('\n',lisa_qry_list[n+1])
  question = ' '.join(line[2:])
  lisa_qry_data[int(line[1])]['question'] = question
```

In contrast to LISA, the NPL only contains ID and text. That simplified the handling of irregular spaces:

```
1
compact memories have flexible capacities  a digital data storage
system with capacity up to bits and random and or sequential access
is described
   /
```

The document, query and relevance assessments file had different newlines around the entry marker which caused some difficulties. With this way of splitting we didn't lose any data:

```python
txt_entry_marker = re.compile('\n   /\n')
qry_entry_marker = re.compile('\n/\n')
rel_entry_marker = re.compile('\n   /\n')

# process the documents

npl_txt_data = defaultdict(dict)

for entry in npl_txt_list:
  splitted = entry.split('\n')
  splitted = list(filter(None, splitted))
  ID = splitted[0]
  text = ' '.join(map(str, splitted[1:]))
  npl_txt_data[ID]['text'] = text
```

### Number Seperators 

Some of the relevance assessment files were difficult to parse, especially with the Time and LISA corpus.

In the LISA corpus we had to be a little more creative, as some of the relevance documents were also stored in the following line.

```
           1           2        3392        3396                                
           2           2        2623        4291                                
           3           5        1407        1431        3794        3795        
        3796         
```

We split the whole file into a list and deleted all empty entries. Since the number of relevant documents is in the second column, we can use this to mathematically calculate at which index the next ID comes and which document IDs are relevant for the actual ID:

```python
# process relevance assesments without rating
lisa_rel = defaultdict(list)

with open (PATH_TO_LISA_REL,'r') as f:
  file = f.read().strip('       ').replace('\n','')
  lines = re.split(' ',file)
  lines = list(filter(None, lines))
  n = 0
  while n < len(lines):
    ID = int(lines[n])
    num_rel = int(lines[n+1])
    rels = lines[(n+2):(n+num_rel+2)]
    lisa_rel[ID].extend(rels)
    n = n+1+num_rel+1
```

The Time corpus has all relevant documents in the same line as the ID, but there are also irregularities here.

```
8  339 358

9   61 155 156 242 269 315 339 358

10  61 156 242 269 339 358

11 195 198

12  61 155 156 242 269 339 358

13  87 170 185

14 269
```

The spaces between the ID and the first document ID vary a lot, so we have to replace the spaces several times with empty strings, split everything into a list and then remove all empty entries:

```python
# process relevance assesments without rating
time_rel = defaultdict(list)

rel_marker = re.compile('            \n        ')
rel_split = re.compile('\n')

with open (PATH_TO_TIME_REL,'r') as f:
  for lines in f:
    line = lines.strip().replace('   ',' ').replace('  ',' ').split(' ')
    if len(line) > 1:
      time_rel[int(line[0])].extend(line[1:])
```

## Fazit

Regardless of which text you are parsing and for which project, it is always important to look at your data and analyze it well. Our guide shows you with examples how you can access such data.
You can find our already parsed data [here](../benchmarks/data-comparisson).