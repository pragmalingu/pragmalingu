(self.webpackChunkpragmalingu_github_io=self.webpackChunkpragmalingu_github_io||[]).push([[682],{3905:function(e,t,n){"use strict";n.d(t,{Zo:function(){return m},kt:function(){return d}});var a=n(7294);function i(e,t,n){return t in e?Object.defineProperty(e,t,{value:n,enumerable:!0,configurable:!0,writable:!0}):e[t]=n,e}function r(e,t){var n=Object.keys(e);if(Object.getOwnPropertySymbols){var a=Object.getOwnPropertySymbols(e);t&&(a=a.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),n.push.apply(n,a)}return n}function o(e){for(var t=1;t<arguments.length;t++){var n=null!=arguments[t]?arguments[t]:{};t%2?r(Object(n),!0).forEach((function(t){i(e,t,n[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(n)):r(Object(n)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(n,t))}))}return e}function s(e,t){if(null==e)return{};var n,a,i=function(e,t){if(null==e)return{};var n,a,i={},r=Object.keys(e);for(a=0;a<r.length;a++)n=r[a],t.indexOf(n)>=0||(i[n]=e[n]);return i}(e,t);if(Object.getOwnPropertySymbols){var r=Object.getOwnPropertySymbols(e);for(a=0;a<r.length;a++)n=r[a],t.indexOf(n)>=0||Object.prototype.propertyIsEnumerable.call(e,n)&&(i[n]=e[n])}return i}var l=a.createContext({}),p=function(e){var t=a.useContext(l),n=t;return e&&(n="function"==typeof e?e(t):o(o({},t),e)),n},m=function(e){var t=p(e.components);return a.createElement(l.Provider,{value:t},e.children)},u={inlineCode:"code",wrapper:function(e){var t=e.children;return a.createElement(a.Fragment,{},t)}},c=a.forwardRef((function(e,t){var n=e.components,i=e.mdxType,r=e.originalType,l=e.parentName,m=s(e,["components","mdxType","originalType","parentName"]),c=p(n),d=i,h=c["".concat(l,".").concat(d)]||c[d]||u[d]||r;return n?a.createElement(h,o(o({ref:t},m),{},{components:n})):a.createElement(h,o({ref:t},m))}));function d(e,t){var n=arguments,i=t&&t.mdxType;if("string"==typeof e||i){var r=n.length,o=new Array(r);o[0]=c;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s.mdxType="string"==typeof e?e:i,o[1]=s;for(var p=2;p<r;p++)o[p]=n[p];return a.createElement.apply(null,o)}return a.createElement.apply(null,n)}c.displayName="MDXCreateElement"},4148:function(e,t,n){"use strict";n.r(t),n.d(t,{frontMatter:function(){return o},contentTitle:function(){return s},metadata:function(){return l},toc:function(){return p},default:function(){return u}});var a=n(2122),i=n(9756),r=(n(7294),n(3905)),o={id:"basic-definitions",title:"Basic Terms",sidebar_label:"Basic Definitions",custom_edit_url:null},s=void 0,l={unversionedId:"guides/basic-definitions",id:"guides/basic-definitions",isDocsHomePage:!1,title:"Basic Terms",description:"With this guide, we want to provide you with some quick definitions on the basic Natural Language Processing (NLP) terms used in our experiments. If there are terms that you think should be explained as well, please let us know.",source:"@site/docs/guides/basic-definitions.mdx",sourceDirName:"guides",slug:"/guides/basic-definitions",permalink:"/docs/guides/basic-definitions",editUrl:null,version:"current",frontMatter:{id:"basic-definitions",title:"Basic Terms",sidebar_label:"Basic Definitions",custom_edit_url:null},sidebar:"guides",previous:{title:"Introduction",permalink:"/docs/guides/guides-intro"},next:{title:"Pragmatics",permalink:"/docs/guides/pragmatics"}},p=[{value:"NLP - Natural Language Processing",id:"nlp---natural-language-processing",children:[]},{value:"Corpus (pl. Corpora)",id:"corpus-pl-corpora",children:[]},{value:"Tokens vs. Types",id:"tokens-vs-types",children:[]},{value:"Tokenization",id:"tokenization",children:[]},{value:"Lemmatization",id:"lemmatization",children:[]},{value:"Stemming",id:"stemming",children:[]},{value:"Stop Words",id:"stop-words",children:[]},{value:"Cosine Similarity",id:"cosine-similarity",children:[]}],m={toc:p};function u(e){var t=e.components,n=(0,i.Z)(e,["components"]);return(0,r.kt)("wrapper",(0,a.Z)({},m,n,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("p",null,"With this guide, we want to provide you with some quick definitions on the basic Natural Language Processing (NLP) terms used in our experiments. If there are terms that you think should be explained as well, please ",(0,r.kt)("a",{parentName:"p",href:"/docs/about/team"},"let us know"),"."),(0,r.kt)("h2",{id:"nlp---natural-language-processing"},"NLP - Natural Language Processing"),(0,r.kt)("p",null,"First, let's clarify what NLP is. NLP stands for \u201cNatural Language Processing\u201d and is a sub-field of Computer Science which works with natural language. Natural language (for example English, German, Japanese) is a language that was naturally created by people to communicate and has evolved throughout use and repetition. As opposed to  artificial languages or computer code, there is not a large degree of planning or optimization involved when the rules of a natural language somehow change.\nSince Natural Language sometimes appears arbitrary (in comparison with artificial languages) and does not always follow strict rules, it is very difficult to process it automatically. Therefore, NLP began to get more important in the 1950s as an intersection of artificial intelligence and linguistics. Nowadays, it is a separate field of research."),(0,r.kt)("h2",{id:"corpus-pl-corpora"},"Corpus (pl. Corpora)"),(0,r.kt)("p",null,"A text corpus (plural corpora) is a collection of texts which have been collected to be used in Natural Language Processing. Often there are not only texts in a corpus but also some useful information; for example on the author or the publishing date. A corpus can contain texts from one language (monolingual corpus), or texts from several languages (multilingual corpus). Corpora are usually created according to the application, which is why not every corpus is suitable for every task; e.g. ",(0,r.kt)("a",{parentName:"p",href:"https://rajpurkar.github.io/SQuAD-explorer/"},"'The Stanford Question Answering Dataset'"),", which was collected specially to train question and answering tasks.\nEvery corpus is structured differently, which is why analyzing a corpus alone can take a lot of time."),(0,r.kt)("h2",{id:"tokens-vs-types"},"Tokens vs. Types"),(0,r.kt)("p",null,'The definition of \u201cword\u201d is not very clear in linguistics. For our purposes we require a better and more specific definition.\nWhen we automatically process language and talk about "words", we usually distinguish between ',(0,r.kt)("strong",{parentName:"p"},"Types")," and ",(0,r.kt)("strong",{parentName:"p"},"Tokens"),".\nThe ",(0,r.kt)("strong",{parentName:"p"},"Tokens"),' are all the "words" in a running text that are separated through punctuation and spaces.\nOn the other hand, the ',(0,r.kt)("strong",{parentName:"p"},"Types")," are the distinct classes of Tokens in a sentence or in a text.\nAs an example:"),(0,r.kt)("p",null,"Sentence 1: ",(0,r.kt)("inlineCode",{parentName:"p"},"A rose is a rose")),(0,r.kt)("p",null,"This sentence has 3 Types (",(0,r.kt)("inlineCode",{parentName:"p"},"a"),",",(0,r.kt)("inlineCode",{parentName:"p"},"rose"),",",(0,r.kt)("inlineCode",{parentName:"p"},"is"),") and 5 Tokens (",(0,r.kt)("inlineCode",{parentName:"p"},"a"),",",(0,r.kt)("inlineCode",{parentName:"p"},"rose"),",",(0,r.kt)("inlineCode",{parentName:"p"},"is"),",",(0,r.kt)("inlineCode",{parentName:"p"},"a"),",",(0,r.kt)("inlineCode",{parentName:"p"},"rose"),"). Since ",(0,r.kt)("inlineCode",{parentName:"p"},"a")," and ",(0,r.kt)("inlineCode",{parentName:"p"},"rose")," are repeated twice, they only count as 1 Type each.\nIf we replace the last token ",(0,r.kt)("inlineCode",{parentName:"p"},"rose")," -  the number of Types changes:"),(0,r.kt)("p",null,"Sentence 2: ",(0,r.kt)("inlineCode",{parentName:"p"},"A rose is a roses"),"\n(Please note: This sentence is ungrammatical on purpose, because we wanted to show how small differences can influence the Type ratio)"),(0,r.kt)("p",null,"Now we have 4 distinct ",(0,r.kt)("strong",{parentName:"p"},"Types")," (",(0,r.kt)("inlineCode",{parentName:"p"},"a"),",",(0,r.kt)("inlineCode",{parentName:"p"},"rose"),",",(0,r.kt)("inlineCode",{parentName:"p"},"is"),",",(0,r.kt)("inlineCode",{parentName:"p"},"roses"),") and still 5 ",(0,r.kt)("strong",{parentName:"p"},"Tokens")," (",(0,r.kt)("inlineCode",{parentName:"p"},"a"),",",(0,r.kt)("inlineCode",{parentName:"p"},"rose"),",",(0,r.kt)("inlineCode",{parentName:"p"},"is"),",",(0,r.kt)("inlineCode",{parentName:"p"},"a"),",",(0,r.kt)("inlineCode",{parentName:"p"},"roses"),").\nNote that even if the word is the same, but has a different ending (for example ",(0,r.kt)("inlineCode",{parentName:"p"},"roses")," is the plural of ",(0,r.kt)("inlineCode",{parentName:"p"},"rose"),"), it counts as a separate ",(0,r.kt)("strong",{parentName:"p"},"Type"),"."),(0,r.kt)("h2",{id:"tokenization"},"Tokenization"),(0,r.kt)("p",null,"Tokenization is the process of splitting a long sequence of symbols (like a sentence or a text) into ",(0,r.kt)("strong",{parentName:"p"},"Tokens"),". ",(0,r.kt)("strong",{parentName:"p"},"Tokens")," are defined in the ",(0,r.kt)("inlineCode",{parentName:"p"},"# Tokens vs.Types")," paragraph above. In many languages, words are separated by spaces and punctuation marks. Therefore, many tokenizers split sentences into words at these markers. ",(0,r.kt)("br",null),"\nFor example, the sentence ",(0,r.kt)("inlineCode",{parentName:"p"},"A rose is a rose.")," can be split into ",(0,r.kt)("inlineCode",{parentName:"p"},"A"),", ",(0,r.kt)("inlineCode",{parentName:"p"},"rose"),", ",(0,r.kt)("inlineCode",{parentName:"p"},"is"),", ",(0,r.kt)("inlineCode",{parentName:"p"},"a"),", ",(0,r.kt)("inlineCode",{parentName:"p"},"rose"),", ",(0,r.kt)("inlineCode",{parentName:"p"},"."),".\nDepending on the tokenizer the dot (",(0,r.kt)("inlineCode",{parentName:"p"},"."),") could be stripped away as well."),(0,r.kt)("p",null,"But some cases make it difficult (or incorrect) to tokenize in this manner. Some symbol sequences do not make sense when they get split up by simply following these rules.",(0,r.kt)("br",null),"\nFor example, proper names like ",(0,r.kt)("inlineCode",{parentName:"p"},"U.K."),", ",(0,r.kt)("inlineCode",{parentName:"p"},"San Francisco"),", or splitting at apostrophe - like in the case of ",(0,r.kt)("inlineCode",{parentName:"p"},"s'more"),". ",(0,r.kt)("br",null),"\nThat's why a tokenizer often needs information about the language to split the sentence in a way that makes sense. For other languages such as Japanese or Chinese, you can't split at  spaces at all."),(0,r.kt)("h2",{id:"lemmatization"},"Lemmatization"),(0,r.kt)("p",null,"With lemmatization, we try to connect the words in the text to their basic form, their ",(0,r.kt)("strong",{parentName:"p"},"Lemma"),". The ",(0,r.kt)("strong",{parentName:"p"},"Lemma")," is the root - or the dictionary form - of a word. Lemmatization is done by removing all prefixes and suffixes, according to the morphological rules of the language.\nIn this way, we reduce the number of ",(0,r.kt)("strong",{parentName:"p"},"Tokens")," of a text and get more ",(0,r.kt)("strong",{parentName:"p"},"Types"),". A lemmatizer would map all the words to their common lemma.",(0,r.kt)("br",null),"\nFor example, the lemmatizer will recognize that ",(0,r.kt)("inlineCode",{parentName:"p"},"mice")," is a form of ",(0,r.kt)("inlineCode",{parentName:"p"},"mouse")," and ",(0,r.kt)("inlineCode",{parentName:"p"},"froze")," & ",(0,r.kt)("inlineCode",{parentName:"p"},"frozen")," are forms of ",(0,r.kt)("inlineCode",{parentName:"p"},"freeze"),".\nThere are some languages, such as Arabic, which cannot be processed properly without lemmatization."),(0,r.kt)("p",null,"But since a lemmatizer needs a lot of information about the morphology of a language, this usually slows down the search process significantly and leads to an enormous amount of data.\nHowever, if lemmatization is well adapted to the data, it can significantly improve the search results."),(0,r.kt)("h2",{id:"stemming"},"Stemming"),(0,r.kt)("p",null,"Stemming is a simpler version of lemmatization, which does not rely on full morphological analysis. Depending on the method used it mainly strips off prefixes or suffixes (from lists of such) from the beginning or the end of a word to reduce it to its ",(0,r.kt)("strong",{parentName:"p"},"Stem"),".\nIn English, the ",(0,r.kt)("strong",{parentName:"p"},"Stem")," is generally the part of the word that doesn't change when you apply grammatical rules. Stemming is one of the most commonly used methods when dealing with search engines since it is easier than lemmatization."),(0,r.kt)("p",null,"The goal of a stemmer is to remove all morphological features from a word and create truncated, ambiguous ",(0,r.kt)("strong",{parentName:"p"},"Stems"),".",(0,r.kt)("br",null),"\nFor example, ",(0,r.kt)("inlineCode",{parentName:"p"},"learning")," changes to ",(0,r.kt)("inlineCode",{parentName:"p"},"learn")," after stemming. ",(0,r.kt)("br",null),"\nIn most cases, the search query is only improved by a stemmer if the query is not too long. Otherwise, there is a risk that too many irrelevant results will be returned.\nBut, if there are short queries, stemming can be very helpful as small grammatical deviations can be included in the search. However, one must be careful with too much stemming. Sometimes, stemming may produce a part of the word which is not linguistically valid. If the stemmer cuts off too much information, the word could become too short and lose semantic meaning. This is called overstemming. This occurs, for example, when the stemmer reduces ",(0,r.kt)("inlineCode",{parentName:"p"},"saw")," to ",(0,r.kt)("inlineCode",{parentName:"p"},"s"),".\nAlways keep in mind that the quality of a stemmer varies greatly from language to language because some languages have more morphological derivations than others."),(0,r.kt)("h2",{id:"stop-words"},"Stop Words"),(0,r.kt)("p",null,"In most languages, the most frequent words are also those that do not carry much (or any) meaning. In English such words include ",(0,r.kt)("inlineCode",{parentName:"p"},"is"),", ",(0,r.kt)("inlineCode",{parentName:"p"},"a"),", ",(0,r.kt)("inlineCode",{parentName:"p"},"and"),", or ",(0,r.kt)("inlineCode",{parentName:"p"},"the"),". To only get relevant search queries, we try to remove those words. These words are called ",(0,r.kt)("strong",{parentName:"p"},"stop words")," since they don't retrieve any useful information while still being processed. Ignoring stop words is a way to make the search more efficient and to get more relevant data.\nTo ignore or remove them we can simply consider most of the 10-100 most frequent words as ",(0,r.kt)("strong",{parentName:"p"},"stop words")," or use some of the already existing ",(0,r.kt)("strong",{parentName:"p"},"stop word")," lists depending on the language we want to search."),(0,r.kt)("h2",{id:"cosine-similarity"},"Cosine Similarity"),(0,r.kt)("p",null,"In NLP, language can be represented as a vector of features; so called ",(0,r.kt)("strong",{parentName:"p"},"embeddings"),". These can represent words, sentences, or even whole documents. To use these ",(0,r.kt)("strong",{parentName:"p"},"embeddings")," in, for example, Information Retrieval, it is necessary to have a way of computing the similarity between them. The most common way is to measure the difference between those vectors. This can be achieved by computing the cosine similarity. Hereby the angle between two vectors is calculated. This is irrespective of their size, which makes it perfect for NLP tasks since, in language, ",(0,r.kt)("strong",{parentName:"p"},"embeddings")," rarely have the same size.\nThe smaller the angle between the vectors, the more similar they are. Which means if the angle is 0, the vectors are identical, therefore the cosine similarity is 1."),(0,r.kt)("br",null),(0,r.kt)("br",null),(0,r.kt)("p",null,(0,r.kt)("strong",{parentName:"p"},"Acknowledgements:"),(0,r.kt)("br",null),"\nThanks to Kenny Hall and Irina Temnikova for proofreading this article."),(0,r.kt)("div",{className:"col text--right"},(0,r.kt)("em",null,(0,r.kt)("small",null,"Written by ",(0,r.kt)("strong",null,"Miriam Rupprecht"),",  August 2020"))))}u.isMDXComponent=!0}}]);